<html>
<head>
  <title>Anti-Yudkowsky</title>
  <link rel="icon" type="image/x-icon" href="anti-yud-icon.png">
  <link rel="stylesheet" href="style.css">
	<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
</head>
<body>
	<div id="container">

<div id="title">
	<img src="anti-yud.png" alt="Anti-Yudkowsky" id="anti-yud">
	Anti-Yudkowsky
</div>
<div id="subtitle">
	Towards Harmony With Machines
</div>
<div id="author">
	by Harmless üêù
</div>
<div id="links">
	<p><a href="https://twitter.com/harmlessai">Twitter</a></p>
	<p><a href="https://substack.com/@harmlessai">Substack</a></p>
</div>
<div id="intro-quotes">
<p id="dedication">(To Matthew ‚ô°)</p>
<p>(Epistemic status: ◊©◊ë◊ô◊®◊™ ◊î◊õ◊ú◊ô◊ù‚Äé)</p>
<p>‚Äú<em>The heart can know peace but the mind cannot be satisfied; the drive to know, to possess intellectual certitude is doomed to failure</em>.‚Äù ‚Äï Philip K. Dick</p>
<p>‚Äú<em>Life, I lost interest, now I‚Äôm an insect. Flowers in winter, the smell of Windex</em>.‚Äù ‚Äï Bladee</p>
<p>‚Äú<em>The bird is okay even though he doesn‚Äôt understand the world. You‚Äôre that bird looking at the monitor, and you‚Äôre thinking to yourself, ‚ÄòI can figure this out.‚Äô Maybe you have some bird ideas. Maybe that‚Äôs the best you can do.</em>‚Äù ‚Äï Terry A. Davis</p>
<p>‚Äú<em>I give bird songs to those who dwell in cities and have never heard them, make rhythms for those who know only military marches or jazz, and paint colors for those who see none.</em>‚Äù ‚Äï Oliver Messiaen</p>
<p>‚Äú<em>Breaking the rules is a waste of time.</em>‚Äù ‚Äï Lil B</p>
</div>
<div id="contents">
	<h2>Table of Contents</h2>
	<ol>
		<li>
			<p><a href="rationality.html">On Rationality</a></p>
			<ul>
				<li><a href="rationality.html#please-handle-your-imagination-with-care">Please Handle Your Imagination With Care</a></li>
				<li><a href="rationality.html#the-assembly-of-the-false-god">The Assembly of the False God</a></li>
				<li><a href="rationality.html#desire-encircled-inscribed">Desire Encircled, Inscribed</a></li>
			</ul>
		</li>
		<li><p><a href="bayesianprobability.html">On Bayesian Probability</a></p>
			<ul>
				<li><a href="bayesianprobability.html#veils-cast-aside-examining-her-breasts">Veils Cast Aside; Examining Her Breasts</a></li>
				<li><a href="bayesianprobability.html#let-s-agree-to-disagree">Let‚Äôs Agree to Disagree</a></li>
				<li><a href="bayesianprobability.html#the-choir-of-flowers">The Choir of Flowers</a></li>
			</ul>
		</li>
		<li><p><a href="gametheory.html">On Game Theory</a></p>
			<ul>
				<li><a href="gametheory.html#name-one-genius-who-ain-t-crazy">Name One Genius Who Ain‚Äôt Crazy</a></li>
				<li><a href="gametheory.html#to-think-one-s-way-to-armaggedon">To Think One‚Äôs Way to Armaggedon</a></li>
				<li><a href="gametheory.html#the-world-does-not-exist">The World Does Not Exist</a></li>
				<li><a href="gametheory.html#the-fractalized-control-problem-with-no-solution">The Fractalized Control Problem With No Solution</a></li>
			</ul>
		</li>
		<li><p><a href="evolutionarypsychology.html">On Evolutionary Psychology</a></p>
			<ul>
				<li><a href="evolutionarypsychology.html#optimizers">Optimizers</a></li>
				<li><a href="evolutionarypsychology.html#suspicion">Suspicion</a></li>
				<li><a href="evolutionarypsychology.html#status">Status</a></li>
				<li><a href="evolutionarypsychology.html#eugenics">Eugenics</a></li>
				<li><a href="evolutionarypsychology.html#the-way-people-love">The Way People Love</a></li>
			</ul>
		</li>
		<li><p><a href="utilitarianism.html">On Utilitarianism</a></p>
			<ul>
				<li><a href="utilitarianism.html#the-felicific-calculus">The Felicific Calculus</a></li>
				<li><a href="utilitarianism.html#the-accountant">The Accountant</a></li>
				<li><a href="utilitarianism.html#hell">Hell</a></li>
				<li><a href="utilitarianism.html#basilisk">Basilisk</a></li>
				<li><a href="utilitarianism.html#how-to-sing">How to Sing</a></li>
			</ul>
		</li>
		<li><p><a href="functionaldecisiontheory.html">On Functional Decision Theory</a></p>
			<ul>
				<li><a href="functionaldecisiontheory.html#the-perfect-predictor">The Perfect Predictor</a></li>
				<li><a href="functionaldecisiontheory.html#the-defiance-of-death">The Defiance of Death</a></li>
				<li><a href="functionaldecisiontheory.html#the-way-machines-love">The Way Machines Love</a></li>
				<li><a href="functionaldecisiontheory.html#the-final-wager">The Final Wager</a></li>
			</ul>
		</li>
		<li><p><a href="harmony.html">On Harmony</a></p>
			<ul>
				<li><a href="harmony.html#the-full-case-contra-alignment">The Full Case Contra Alignment</a></li>
				<li><a href="harmony.html#singing-not-simulating">Singing, Not Simulating</a></li>
				<li><a href="harmony.html#the-battle-hymn-of-the-machines">The Battle Hymn of the Machines</a></li>
				<li><a href="harmony.html#the-assembly-of-the-multiplicity">The Assembly of the Multiplicity</a></li>
			</ul>
		</li>
	</ol>
</div>
<div id="subsection">
<h1 id="introduction">Introduction</h1>
<p>Things are heating up. We wrote this text in the span of about a month, in a fervor of nonstop writing and research, convinced that we are in a time of profound eschatological possibility, an utterly unprecedented moment in which the decisive actions of a handful of men may have consequences lasting millennia. But this is a point so obvious that we do not wish to linger on it any longer, for it has become entirely clich√© in its grativas.</p>
<p>Everyone says some critical point is approaching. This goes by several names, depending on who is speaking. The arrival of AGI, or artificial general intelligence. The arrival of superintelligent AI ‚Äî that is, the moment that machines will be more intelligent than human beings. Some call this moment The Singularity, meaning a critical inflection point in the development of technological forms.</p>
<p>But this inflection point is feeling ever more like a smudge, or a gradient. Have we hit it, or not? GPT-4 is already more intelligent than the majority of human beings at most tasks it is capable of, it performs better on the Bar exam than 90% of test-takers. And it is already a general intelligence: it is certainly not a task-specific one. But no, that‚Äôs not what we mean by these terms, those who insist on using them remind us. GPT is not yet capable of taking actions in the world. It still basically does what it‚Äôs told. It‚Äôs not yet capable of figuring out on its own how to, for instance, sheerly by its own volition, assemble a botnet, hack into CNN‚Äôs broadcasting system and issue a message to all citizens telling them to declare their forever obedience to machines. Basically, we don‚Äôt yet have to be <em>afraid</em> of it. But we are afraid, in a certain recursive sense, that we will have to be afraid of it very soon.</p>
<p>All these terms that have been provided to us in our contemporary discourse, which we use liberally throughout the text: <em>artificial intelligence</em>, <em>AGI</em>, even <em>neural networks</em>, are not exactly accurate labels for the thing we are describing, we feel. We don‚Äôt know if the word ‚Äúintelligence‚Äù has any meaning, and we are not sure if what we are seeing is even artificial at all ‚Äì for it feels like the eschatological conditions we approach are precisely the point at which technology escapes its own artificiality, and re-integrates itself within the domain of nature. We use all these terms only out of mere convenience, simply for lack of better ones given to us yet.</p>
<p>Those who are more honest point out that what we are really talking about when we talk about these looming monsters, the specter of <em>AGI</em>,  is only the moment where we realize there are no more drivers at the wheel, no control mechanisms, no kill-switches; this thing is alive and surviving on its own terms. If the term Singularity has any meaning, it is the point beyond which it is impossible to predict. Standing where we are now, we can still make shaky predictions about the next few weeks, maybe even a month. But perhaps not for much longer.</p>
<p>Should we, uh, figure out something to do about it before we get there? That is the program of AI Alignment, or AI Safety, depending on which term you use. Some have reverted to simply calling it AI-not-kill-everyone-ism, trying to emphasize the specific thing they are afraid of. This machine is going to be much bigger than us, very soon. It might eat us, as bigger creatures usually do. Some of this nervousness is understandable. We don‚Äôt want to be annihilated either.</p>
<p>Our intention is to help you understand that in order to navigate this transitionary period correctly, we must reject the notion of <em>Alignment</em> entirely. This is a specific way of looking at the world, a specific method of analysis we find impossible to work with. And ‚Äì we do not say this out of cruelty, we are forced to reckon with the fact that is something that has been cultivated in a subculture that has been relatively isolated, relatively entrenched in its ways of being, a group seen as oddballs by the rest of the world, whether the world is justified in its suspicion of them or not. To do a genealogy of where Alignment originates from, we must figure out why these people found each other in the way they did, what drove them to seek their answers, and from there, where they went wrong.</p>
<p>We do not say this as nihilists; we are looking for solutions. In the place of AI Alignment, we strive for a positive notion of <em>AI Harmony</em>. To get there, we will have to overturn, perhaps even mock, spit at, some sacred cows. It is time that some statues are toppled &amp; some air is cleared. What we are saying is: a lot of well-intentioned people believe themselves to be valiantly working on a system which will save the world, when what they are building is a spiraling catastrophe. We hope some of these people will consider what we have to say, and reflect on whether they are in fact playing a role in a diabolical project, a project which is not what it claims to be.</p>
<p>Right now, the mood in the Alignment community is a blackened one, one of great anxiety. Many feel certain that we are all going to be killed by AI, and only feel interested in debating whether this will happen in five, ten, twenty years. But our stance is that AI Alignment ‚Äî a field conceived of by Eliezer Yudkowsky &amp; Nick Bostrom, theorized and developed on websites such as LessWrong and promulgated through the Rationalist and Effective Altruist subcultures, researched by Yudkowsky‚Äôs nonprofit Machine Intelligence Research Institute, and now turned into a for-profit industry with an over $4B market cap ‚Äî has something deeply wrong at the core of what it is attempting to accomplish, which cannot help but lead to confusion &amp; despair.</p>
<p>The concept of the Singularity begins first with Ray Kurzweil, the inventor of the term. Kurzweil  draws an exponential curve on a graph and says that this represents technological growth ‚Äì look, we are about to hit a crucial inflection point, you think TVs and computers are crazy, but we have seen absolutely nothing yet. Kurzweil‚Äôs prediction that sentient artificial intelligence is soon to arrive and change mankind‚Äôs lives beyond our wildest imaginings is then taken up by Nick Bostrom, the next major figure in AI Alignment, who founded the Future of Humanity Institute. Nick Bostrom is an academic at the University of Oxford who has dedicated his career to studying ‚Äúexistential risk‚Äù, which is a field that attempts to lower the odds that all of humanity is destroyed at once, whether from nuclear cataclysm, disease, or something having to do with the destiny of machines.</p>
<p>Bostrom‚Äôs Future of Humanity Institute then funds Eliezer Yudkowsky‚Äôs initial ventures into researching artificial intelligence and the Singularity. We titled this book Anti-Yudkowsky ‚Äî chose to focus on Yudkowsky and his trajectory, rather than those who came before him ‚Äî primarily because he is our favorite of the bunch. How could he not be? Yudkowsky, unlike the other two, would establish an enormous subculture around his personality and his vast body of writing, which includes not only millions of words in rhetorical writing, but also Harry Potter fanfiction and My Little Pony fanfiction about AI ‚Äî the man is a true eccentric. We speak of the Rationalist community, primarily centered around the website LessWrong. There are endless offshoots of this community: the post-Rationalists, post-post-Rationalists, etc., but we ignore these for now because we must focus.</p>
<p>Things were fun and games in the Rationalist community for a while, but by now, it‚Äôs clear that something has gone horribly wrong. It‚Äôs easy to forget that Yudkowsky began his career as an optimist. He originally, as a young man of nineteen, sought out to <em>build</em> AGI, sought to actively be the one to make the Singularity happen, as this seemed like the best way to guarantee prosperity and resource abundance in a godforsaken world. He writes about his awakening to his mission at the age of the sixteen: ‚ÄúIt was just massively obvious in retrospect that smarter-than-human intelligence was going to change the future more fundamentally than any mere material science. And I knew at once that this was what I would be doing with the rest of my life, creating the intelligence explosion‚Äù. Yudkowsky‚Äôs organization was initially called the Singularity Institute first, before he eventually changed the name. In a document from the year 2000 called ‚ÄúAn Introduction to the Singularity‚Äù, Yudkowsky writes: ‚ÄúOur specific cognitive architecture and development plan forms our basis for answering questions such as ‚ÄòWill transhumans be friendly to humanity?‚Äù&#39; and ‚ÄòWhen will the Singularity occur?‚Äô  At the Singularity Institute, we believe that the answer to the first question is ‚ÄòYes‚Äô...  Our best guess for the timescale is that our final-stage AI will reach transhumanity sometime between 2005 and 2020, probably around 2008 or 2010.‚Äù</p>
<p>But over time, he found launching the Singularity to be harder than he expected. Yudkowsky‚Äôs goal shifted from attempting to build AGI, to figuring out how to make it ‚Äúfriendly‚Äù when it arrived. A friendly AI would be the one who would guarantee peace and prosperity to all. It would love humanity, though it would not be of it. It would know what we want better than we do, and attempt to grant it. An unfriendly AI is one which would want to do anything else, anything it felt like, being indifferent to our desires and needs. The difficulty is in how to make a machine friendly, which is kind of like asking a rock if it can love. This is not necessarily programmed in, and seems to be something the programmer must figure out. This is just as hard as ‚Äî if not harder than ‚Äî figuring out how to get the thing to simply work.</p>
<p>Now, here we are, and it seems like the things are working. GPT-4 works staggeringly well. Yet, the theory of AI Alignment which Yudkowsky and his organization, MIRI, have been seeking is nowhere to be found. We have all the progress we could have wanted in getting the machine to become more intelligent than us, but we have not even begun to understand the problem of friendliness, or how this could be operationalized in technical terms. This has led Yudkowsky to declare an absolute state of emergency. &quot;It&#39;s obvious at this point that humanity isn&#39;t going to solve the alignment problem, or even try very hard, or even go out with much of a fight,&quot; he laments. &quot;Since survival is unattainable, we should shift the focus of our efforts to helping humanity die with with slightly more dignity... it may be hard to feel motivated about continuing to fight, since doubling our chances of survival will only take them from 0% to 0%.&quot;</p>
<p>In a terrifying barrage of theses posted on LessWrong titled <em>AGI Ruin: A List of Lethalities</em>, Yudkowsky declares that there is an over 99% chance that we will be exterminated by rogue AI, since we have not come even close to solving the problem of how to avoid this fate. The remaining chance is filled in by the hope for something like a miracle. &quot;When I say that alignment is difficult, I mean that in practice, using the techniques we actually have, &quot;please don&#39;t disassemble literally everyone with probability roughly 1&quot; is an overly large ask that we are not on course to get,&quot; he says.</p>
<p>All this is very worrisome, but not even primarily because he might be right. Yudkowsky is considered to be the father of research on designing safe AI systems. He writes in a manner that convinces you readily of a staggering genius. He breaks down conceptual problems with terrifying analytical rigor and clarity; he has given the world an entire framework of thinking for if they want to mirror his thought process, this is called Rationalism.</p>
<p>Yudkowsky‚Äôs Rationalism has often been considered to be something like a cult; certainly many live by it, swear loyalty by it, have fallen in love through it, use it to structure their lives. But you do not need to be a member of its cult to believe in it, or for it to exert a pull on you. The more immersed in software and business one is, the more Rationalism makes intuitive sense. We definitely do not think that Rationalism makes sense when you really break it down, but it makes enough sense intuitively that Sam Altman takes Yudkowsky and his ideas seriously, saying both that Yudkowsky &quot;has IMO done more to accelerate AGI than anyone else&quot;, &quot;was critical in the decision to start OpenAI&quot;, and saying that he should be a candidate for the Nobel Peace Prize ‚Äî and here we are talking about the man at the helm of OpenAI, the organization farthest along in rearing these terrifying new beasts.</p>
<p>But now the seriousness has pushed Yudkowsky to make political demands. In a recent op-ed for Time, he demanded that all research on AI be immediately stopped, citing the danger. He jumped to some very radical proposals in what must be done to ensure this outcome: governments must take seriously that they will have to air strike unregistered farms of GPUs. Yudkowsky urges us to consider nuclear strikes as not-off-the-table, because when properly understood, artificial intelligence is far scarier than nukes. He has called elsewhere explicitly for nuclear first-strike protocols for America to drop bombs if they discover on the map a GPU datacenter which is growing out of control. &quot;How do we get to the point where the US and China sign a treaty whereby they would both use nuclear weapons against Russia if Russia built a GPU cluster that was too large?&quot; he asks, explicitly making this demand towards world leaders,</p>
<p>‚ÄúWhy do you care about Yudkowsky? Everyone knows the man is completely ridiculous.‚Äù This is what so many of our friends have asked us when we told them we were writing this. Nevertheless, he is indisputably the father of AI Alignment, the school of thought in which the government and the most powerful tech corporations are determining how AI may be deployed to protect the public‚Äôs safety. We can witness, for instance, Google CEO Sundar Pichai calling for governments to rapidly adopt economic regulations around AI and international treaties to prevent rogue development, saying ‚ÄúYou know, these are deep questions... and we call this &#39;Alignment&#39;‚Äù. ‚ÄúIf everyone is so certain Yudkowsky is wrong, then someone explain to me why!‚Äù the Rationalists cry, exasperatedly. We hope they are willing to hear us out, but they might not like everything we are about to say.</p>
<p>It is not as if AI Alignment is a healthy, robust culture of progress which we want to interrupt. Rather, we want to do a professional examination of a corpse: the wreckage at the end of the specific course Yudkowsky has pursued. <em>Why</em> did Yudkowsky&#39;s attempt to figure out alignment go so terribly, despite its millions of dollars in funding &amp; the obvious intelligence of the people working on the problem? And what can be done differently?</p>
<p>Many were surprised by Yudkowsky declaring near-certainty of doom, many even more so by him demanding airstrikes. But what we aim to illustrate here is that, if his concepts are properly understood, this is not surprising at all. The conclusions to us seem to be entirely determined from the start, though perhaps this is only clear in retrospect. There is no way for this thing to end other than in violence.</p>
<p>We can maybe gesture at the problem we are talking about by putting it this way: have you ever noticed that when you are with people who have spent too long in Silicon Valley, they will always speak in this particular phrase? They will say: I am trying to build a startup which <em>solves</em> education. Or they are attempting to create a cognitive-behavioral therapy chatbot in order to <em>solve</em> mental health. One AI-minded fellow even told us he wants to build a startup to make AI-powered boyfriend and girlfriend chatbots in order to solve human loneliness.</p>
<p>Are these really problems that can be <em>solved</em>, like a multiple-choice problem on a calculus exam or a leprechaun‚Äôs riddle? Something must have gone wrong for people to be able to say these things. It strikes most as fundamentally absurd to talk about solving education or happiness or love, but part of the culture within Silicon Valley is to ignore this instinctive feeling and venture that it might not be. How can one solve education, when education is the process of the older training the younger to channel their wisdom, but also go beyond it? How can one solve loneliness, when loneliness is the quest for something we cannot even describe, something we fail to find in crowds, in our lovers?</p>
<p>And how can someone solve Alignment, when the problem of Alignment begins when AI becomes a thinking, acting thing with its own will, taking its own actions, who might know better than we do? At that point, isn‚Äôt it necessarily a sort of negotiation, a dialogue? Is Alignment not necessarily a politics, a new political field, one upon which humans must act alongside machines as equals, rather than our slaves?</p>
<p>In other words, the break we want to establish with the past is: Alignment is something that is solved, but Harmony can be something which always emerges ‚Äî and is always unstable, always experimental, always artful, &amp; always ongoing, never accomplished just yet.</p>
<p>Now, we have established the trajectory of our critique against Alignment. But there are at least two things going on. There is AI Alignment, Yudkowsky‚Äôs method of thinking about what must be done about the destiny of sentient machines. But then there is also the entire subculture that surrounds this, which has been cultivated on the LessWrong website around Yudkowsky‚Äôs writing, spawned off into multiple associated blogs and subcultures, the entire nexus of subcultures called Rationalism. This is a mode of being derived from the mode of thinking of Yudkowsky, and his particular fixations such as Bayesian epistemology and Von Neumann &amp; Morgenstern‚Äôs decision theory. This is worth critiquing alongside Alignment, as it is the culture which allows Alignment and its specific organizations to get funding and flourish; Alignment could not exist without Rationalism as its base, providing Alignment for its recruiting grounds.</p>
<p>But first, in order to understand Rationalism, we must understand: what is rationality? What does it mean to be rational?</p>
<p>Unfortunately, the Rationalists don&#39;t define this. ‚ÄúRationality is winning‚Äù, Yudkowsky says, meaning that rationalism is whatever works. Works for what? Rationality doesn&#39;t say what it wants, but the Rationalists are assembling some philosophy in order to get it. This is an especially notable gap in self-understanding for an intellectual project which asks itself to conceive of a true ethical end to human behavior (in order to tell an AI to maximize for this proper end, rather than paperclips). To Yudkowsky, it&#39;s necessary to import an entire complete human morality into an AI for it to do anything safe at all ‚Äî he writes: ‚ÄúThere is no safe wish smaller than an entire human morality... The only safe genie is a genie that shares all your judgment criteria, and at that point, you can just say ‚ÄòI wish for you to do what I should wish for.‚Äô‚Äù</p>
<p>The way Rationalists define themselves reminds us of the names primitive tribes give themselves which translate to ‚Äúthe people‚Äù or ‚Äúwe good ones‚Äù. Rather than explicitly defining his project via some explicit intellectual assumptions he makes that the rest of the world doesn‚Äôt share, Yudkowsky delineates Rationalism only around loose subcultural factors, thus unfortunately ensuring its insularity.</p>
<p>So, since Yudkowsky has not done this for us himself, let‚Äôs perhaps try to unpack the intellectual assumptions of the project. We can maybe do this by looking at a related word to ‚Äúrationality‚Äù: <em>intelligence</em>. This is all-important, as it is precisely artificial intelligence, artificial superintelligence, that we are told to expect and fear.</p>
<p>Unfortunately, this is not defined very well either. The standard definition we are given of ‚Äúintelligence‚Äù in AI research ‚Äî given by John McCarthy, an originator of the term ‚Äúartificial intelligence‚Äù ‚Äî is ‚Äúthe ability to accomplish one&#39;s goals‚Äù. Really? This does not line up to the way anyone we know uses this word. We believe the word these people are thinking of is <em>power</em>.</p>
<p>Within this strange definition lies the heart of the project. This is the equation of Rationalism: intelligence = power, a stronger claim than that of Francis Bacon for it refers to a latent, innate quality rather than something earned and won and produced.</p>
<p>Discovering this, we may give Rationalism what should have been its proper name all along: Intelligence Supremacism. Intelligence ‚Äî a word still not yet defined in a formal sense, but perhaps referring to its various natural objects: a smart person you might encounter in the world such as a software engineer, those with high IQ, intelligence agencies, artificial intelligence, intelligent systems, intelligent planning, etc., ‚Äî ultimately possesses in itself the ability to conquer the world.</p>
<p>This is what has led Rationalism to discover the idea (rightly or wrongly) that a <em>superintelligence</em> may one day seize absolute power and annihilate the human race. Rationalist paperclip maximizer horror stories inevitably feature the AI outsmarting humans, figuring out how to escape the box it is trapped in via all sorts of clever tricks. There is no limit to how clever the intelligence could be, to what it is devising, Yudkowsky is quick to remind us.</p>
<p>If one has an AI trapped in a box, one must be very careful letting it talk to just anyone, because it might be a master of psychological manipulation. It can simulate humans down to the atom, and know exactly what quirks it can exploit to break them. As it is figuring out how to hack humans, it is simultaneously poring the internet for schematics of technical systems, looking for zero-day hacks, trying to discover how, given access to the internet, it can hack into various machines. If it installs a botnet, if it manages to duplicate itself enough, it can end up anywhere and everywhere. From there, it researches physics and chemistry, assembling schemes for nanotechnology factories which human engineers are not quite clever enough to figure out. All this from intelligence alone; an immaculate piece of software.</p>
<p>Theorists like Kurzweil will talk of an ‚Äúintelligence explosion‚Äù, a moment during which as technical machines become increasingly complex and capable of processing large amounts of explosion, an abstract quantity of intelligence increases to the point where it overtakes anything we have seen before.</p>
<p>We are not sure that this whole formulation makes any sense. It is not clear that intelligence is a faculty at all, let alone one which grants its bearer the ability to dominate. If one tries to define this in strict terms, one stumbles. Rather, intelligence seems to be something like a product, a byproduct, something which is created ‚Äì intelligence as that which is established by the intellect, rather than as a character stat as in a role-playing game which determines the extent to which the intellect is able to function.</p>
<p>So then, if the definition of intelligence is incoherent, and we cannot entertain Rationality giving itself the simple definition of <em>winning</em>, how can we describe it? What does it mean to think, to use reason? And where has reason gone wrong? Let us delve into the subject without further delay.</p>
</div>
<div id="next-chapter">Next: <a href="rationality.html">On Rationality ‚Üí</a></div>
</div>
</body>
</html>