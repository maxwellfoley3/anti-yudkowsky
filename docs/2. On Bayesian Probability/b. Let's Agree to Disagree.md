## Let’s Agree to Disagree
##### **(Bayes in Practice)**

First, let’s consider if it is useful for machines. State-of-the-art neural networks — let’s say for instance, GPT-4 can be described as such: GPT-4 is a complex matrix algebra formula which predicts the next word in a text via inputting a matrix representing the existing text thus far and outputting a number representing the next word in the text.

The form of the GPT-4’s equation is defined by the engineer in advance, but its constant factors are not, and must be *learned* in the training process. To help the reader understand this: the process of training GPT4 is like a more complicated version of some statistical problem where we believe an equation like y = Ax<sup>3</sup> + Bx<sup>2</sup> + Cx will predict y for a given x, but we must determine the best A, B, and C to discover this equation. In the case of GPT-4, there are over a trillion such A B and Cs. Finding the values for these is what is called learning.

How do we learn A, B, and C? In its training phase, GPT-4 repeatedly tries to guess the next word, and initially it gets it wrong every time. But each time it fails, we can run a calculation to say: which A, B and C etc would have potentially gotten it right? This is called gradient descent. We then push our estimation of A, B and C etc slightly towards the direction of the values that would have been correct in this context (this is called backpropagation), and try again.

The better this predictive system performs, the more it approaches the ideal of a Bayesian reasoner, but this is tautological. Is GPT-4, in its design, modeled after an ideal Bayesian predictor? Not especially. There are explicitly designed Bayesian Neural Networks, but these are for more special purposes, because, as described above, the explicit updating over all possible worlds a Bayesian reasoner must implement is not computationally feasible for anything other than very succinctly defined domains.

GPT-4 updates its truth-notion not with the formal, precise accuracy of Bayes’ formula, but in fits and bursts using gradient descent. GPT-4 takes leaps forwards and backwards. As for the structure of its truth, do GPT-4’s A, B, C, and trillion other parameters describe algorithms for possible worlds? The researcher Janus believes that they do and has argued for this in their post Simulators and elsewhere, but we at Harmless are not entirely convinced... What these numbers encode, what GPT understands as its truth, seems like a profound question to wrestle with, which we might touch upon later.

So it is seen that in the realm of machines, we must make speculative jumps rather than explicitly use Bayes’ formula to update our predictions. What about with humans?

Among Rationalists, amongst the Bayesian community, they will occasionally recommend crunching Bayes’ formula to make some prediction, whether about one’s personal life or some global event. But it is said that what is more important is internalizing the felt sense of Bayes’ formula so that one can reason while conceiving of it as an ideal. Bayes’ formula is, at the end of the day, a vibe you pick up on.

What does this mean? Again, the imperative to become a Bayesian reasoner is the imperative to continuously construct the grounds for probabilistic determinations. The Bayesian reasoner must see himself as someone who knows his priors — he possesses his distribution of prior probability. And when challenged on his expectations of the world, he must present it as a probabilistic claim.

This becomes a set of Rationalist community epistemic norms. When among Bayesians, act like a Bayesian reasoner. Rationalists will ask you “what are your priors?” and it is rude not to answer. For any truth claim you output, they will ask you “What is the probability that this is true?” — no truth claim may be served without this. It is polite to put a probability value, the “epistemic status”, at the beginning of any Rationalist essay you might write.

Rationalists describe a postulate derived from the axioms of Bayesian reasoning called Aumann’s Agreement Theorem which says that any two Bayesian reasoners, assuming goodwill, must eventually converge on an identical set of prior probabilities. When disagreeing with a Rationalist, the most important question becomes what aspect of one’s priors led the disagreement to occur in the first place. Any deviation from a potential shared set of priors means that one person must be held in the wrong. The disagreement should be reconciled quickly, otherwise there is a possibility for pollution in the epistemic commons. To be wrong for too long is considered potentially dangerous, as one falsehood begets another through a chain of corrupted priors, and the picture of reality becomes smudged. It is imperative that when disagreement happens, the interlocutors find the precise point of divergence so that they may re-align. For someone to spend time reading a long-winded critique, one which challenges fundamental assumptions and spends time elaborating upon bizarre metaphors, is to deviate from the efficient ideal of Bayesian reasoning, and if the Rationalist reading this is still with us, we thank you for your patience. 

Something very remarkable happens once people start acting this way. It is as if the community itself strives to become the artificial Bayesian Neural Network which GPT-4 for example is not; a collective hivemind that forwards predictions to each other to produce a sense of reality, a prior distribution upon which one can make predictions, which the Rationalists for instance do on the prediction aggregator Metaculus. As we have said, it is like as if to figure out how to outmaneuver the emerging superintelligence, the Rationalist community *must first become a superintelligence themselves*, the only way we know how, by aggregating the power of many high-IQ human brains.

Rationalists have a very strong sense of their own exceptionality as a community; it seems they feel like they are the only ones capable of uncovering truth. If to act collectively within these norms of Bayesian reasoning is the ideal way to uncover truth, then this is true, for they are the only semi-organized group who acts this way, at least that we know of.

It’s interesting to note that goodwill between the nodes in the Bayesian Network is necessary to perform this process. If someone is duplicitious, or dismissive, or excessively disagreeable, they cannot perform the proper function of forwarding information within the hivemind. As such, it must be the case that people within Rationalism share certain goals. It must be a curated space free from foundational conflicts. There is a remarkable essay by famed Rationalist Scott Alexander called “Conflict Theory v. Mistake Theory” in which he contrasts two theories of disagreement, one in which people disagree because of deviating beliefs about reality which they can resolve, and one in which people disagree due to conflicts. After spending so long immersed in the politics-free Rationalist space in which Bayesian reasoners with remarkably little drama work on gradually converging on their shared set of priors so they may coordinate action, Scott realizes with a sort of shock that most people exist in a world where political disagreement arises from inextricable conflicts (such as competing claims on shared resources, national and class antagonisms, etc). This leads to a situation where truly competing wills are not present in Rationalism. One can entertain a lot of bold proposals in a Rationalist space, but if one is committed to the idea that, for example, libertarian capitalism is a bad axiom, or that software-engineering types should have less power in the world rather than more, one is not able to integrate oneself into Rationalism.

As such, the Rationalist community, despite its thriving debate and blogging culture, is not exactly a forum for open, free, unguided inquiry like an Athenian gymnasium or Enlightenment coffeehouse or French intellectual salon. The Rationalist community is an hivemind constructed for the purposes of something — what exactly? *Rationality is winning*, but winning at what? It depends on who you ask, for some Rationalists it is merely to increasingly cultivate the art of rationality: increasingly honing its own powers of superintelligence, suspending the moment where it gets applied to a particular task. For some Rationalists it is just to make friends. For Yudkowsky, it is to establish a community of people who think like him so that he does not need to solve the AI Alignment problem alone.

How has Rationalism fared at this so far? In its initial days, it seemed as if the Rationalists believed that their methods of reasoning would give them near-superpowers and allow them to take over the world very quickly. Scott Alexander wrote an entertaining post in 2009 titled “Extreme Rationality: It’s Probably Not That Great” urging them against some of their boundless optimism with respect to their methods. But there have since been some attempts at Rationalists to gain serious power — exactly which ones qualify probably depends on finding some difficult boundary of what counts as a true Scotsman. Is Vitalik Buterin a Rationalist? Is Sam Bankman-Fried?

It’s clear that Rationalism failed in its primary task of allowing Yudkowsky to form a collective mind capable of solving Alignment alongside him. In *AGI Ruin*, in which he declares despair over Alignment and predicts a >99% chance of death, he repeatedly bemoans the fact that he is “the only one who could have generated this document” and that “humanity has no other Eliezer Yudkowsky level pieces on the board”. “Paul Christiano’s incredibly complicated schemes have no chance of working”, he laments about one of his closest collaborators. There are not many truths that Rationalism collectively discovered that it did not know at first, nor is there anything it radically changed its mind on. And while Rationalism’s founder, Yudkowsky, has declared a >99% chance of death from AI, few in this community are updating from his posteriors to go along with him, or can even really feel like they understand fully where his confidence comes from, much to his great frustration. Rationalist epistemic norms have allowed for a lot of riveting debate, great writing, and the formation of many friendships, but it’s not clear that people actually converge on a ground of priors, or that performing the speech-patterns of a Bayesian reasoner actually allows one to approach the ideal one is approximating. People don’t usually end up finding a common ground when they debate — usually they end up relaxing parts of their position while bunkering into some increasingly pedantic and obscure point until the other person gets bored. Disagreement doesn’t get resolved, it spirals endlessly. The tree-like structure of the LessWrong and Astral Codex Ten comment sections reveals this all too well. People aren’t especially exchanging a set of probabilities over truth-claims when they discourse, least of all in the fluid, low-friction manner expected of a network. What people mostly do is quibble over a set of linguistic frames.

What can be done? Is it possible to construct something more optimal? We feel that the failure of the Bayesian community to come to a healthy consensus arises from this structure it places upon the operation through which one perceives, investigates, learns from the world, uncovers reality. Knowledge of reality is held to be ability to model it as an algorithm which generates one’s experience. But there is something rather hubristic about this idea: that in order to understand reality and be guided by it, one must also fit it inside one’s head. 