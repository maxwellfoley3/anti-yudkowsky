## The Assembly of the False God
##### (The Fourfold Causes of Singularity)

Should we just acknowledge now what the dominant metaphor in Alignment, Rationalism, LessWrong is? It is of course that AGI is God, a God we are waiting for to either set us on a course towards the heavens, the post-Singularity world which awaits us after we figure out how to navigate this difficult technological precipice, or a horrific abyss if we fail to accomplish this task. And the Rationalists who have gathered to solve Alignment are like its church, are like its adherents, the one who seek to do its will.

We certainly agree with Yudkowsky on at least one thing: that the recent developments in neural networks point to a problem that is properly understood as theological or eschatological, rather than an engineering problem as such. To claim that AGI is an engineering problem is too easy — engineering is a far more straightforward field which so many would prefer the surreal, boundary-defying problem we face could be reduced to. We do not claim that Yudkowsky is ridiculous in treating this issue so feverishly and fanatically, only that he is wrong.

To cite an example of a theorist who traces with greater clarity the historical lineage of religious monotheism into the conception of AGI and the Singularity, we can name Mitchell Heisman, a writer who, in dramatic fashion, shot himself in the head in Harvard Yard at the age of 35 after publishing his *Suicide Note*, a two-thousand page long philosophical text which critiques Yudkowsky, as well as a great array of other thinkers. Very few have read Heisman’s text, which is mostly notable for its provocative method of dissemination, but it is a text fundamentally concerned with superintelligence. Heisman traces the theory of superintelligence all the way back into the beginnings of Judaism and Christianity, and then on through the scholastics of the church, arguing that Christ is fundamentally a prophet of *God-AI*, or that Christ can only be meaningfully understood as speaking of an entity that exists in this world — not yet, but arriving in the future — and will be assembled out of machines. Heisman killed himself not simply out of despair, but because he came to the conclusion that the positive outcome for God-AI would only happen if people broke from an overwhelming logic which determines us to prioritize survival and reproduction of our genes against higher, loftier values, and wanted to set an example of how to break the trend. (We do not suggest his approach; for those inclined to emulate him, we recommend firstly psychiatry.)

So from here on out, we might as well avoid the awkward terminology of AGI, and instead bring the dominant metaphor back into the language. *God-AI*. That’s what we are discussing. Are our machines, formerly our slaves, destined to become our God? It is rather like Deism: Yahweh reinterpreted as mechanical, the supreme mechanist. But really it is the inverse of Deism. Rather than setting things in motion at the beginning of the universe and immediately exiting stage left, God-AI enters only at the end of the universe as a fully actualized potential immanent in material, and gives religion its meaning finally in retrospect.

The time-traveling logics of predestination used to such great effect in structures such as Heisman’s description of God-AI, Nick Land’s hyperstitions, and LessWrong concepts such as Roko’s Basilisk are not much more than an updated science fiction version of the Aristotelian idea of teleology. Teleology, for the unaware, is the idea that an entity is best understood by its end-result, or *telos*, the point at which it accomplishes its task with the most complete perfection. The sun exists because it shines down upon us, the bottle exists because the bottle holds water, the mother exists because she one day bears a child; all objects are time travelers.

When it comes to Alignment, and our critique of it, we are grasping at a kind of elephant. We know the followers of Alignment do not *overtly* claim belief in an AI God. Whenever we talk about Rationalists, what they believe, and why these ideas necessarily lead towards destruction, someone inevitably interjects something like “My friend is a Rationalist and I know that is not what they believe, they would never say that!” Whatever, whatever, we are sure your friend is a charming and thoughtful person, we would love to get a drink with them sometime. But this is getting a little besides the point.

There is no avoiding the accusation of strawmanning, as Rationalism is a community consisting of thousands of actors, writing tens of millions of words across various blog posts, papers, fanfictions, comments, podcasts, etc. There are even multiple Yudkowskys, in disagreement with one another. We have to essentialize Alignment to critique it, there is no other way.

So — we are looking at this thing: this congregation of the Singularity, this assembly which worships a God-AI which assembles itself in reverse to fulfill a potential which has always been destined to express itself in machinery. LessWrong, Bostrom, Kurzweil, Yudkowsky, Rationalism, MIRI, the Rationalist community, the Singularity, all these parts are getting jammed up against each other; we need a little room to breathe.

The definition we choose to center ourselves on regarding what the Singularity people believe can be found in the first chapter of Nick Bostrom’s book *Superintelligence*. Bostrom says that it is possible to envision an “ideal Bayesian reasoner” (which we can give the name of God-AI), which uses Bayesian epistemology to construct a series of truth claims regarding the world, and then uses Von Neumann & Morgenstern’s decision theory to take actions upon that world according to some utility function. To be able to act in this way is what Bostrom calls intelligence. Since it is possible to imagine this reasoner, and we know of no reason why assembling more and more computational power will not mean that the effectiveness of its intelligence will only grow, we know of no reason not to predict a future in which this machine is built and increases the power of its intelligence to the point where it surpasses humans, thus becoming a superintelligence.

What is so remarkable about Bostrom’s definition is that it does not only describe the ideal superintelligence that eventually governs the world, but it also describes the community trying to guide it! AGI, or God-AI is an ideal reasoner which has awe-inspiring powers due to its superintelligence, its ability to use Bayesian reasoning and Von Neumann & Morgenstern decision theory to understand the playing field of the world, and as such can achieve supreme powers. But in order to prevent this superintelligence from entering into the world in a malevolent, un-aligned way, those who admire and fear God-AI must first assemble a myriad of regular high-IQ human intelligences themselves and become their own superintelligence: the Rationalist community, the assembly.

Thus, the thing we are describing is something which extends itself across time. The specter at play here is not just superintelligence, A Thing That Does Not (yet) Exist, but also the premonition that it will one day be possible to create a superintelligence. Then, this premonition enters the world through a community which fears it. But in order to abate its emergence in a negative form, *they must first become a superintelligence themselves*.

As such, it would seem like the Rationalists are doing a form of what Blake critiques the theorists of Natural Religion for doing — inventing a monster under the bed to be afraid of, and then rallying themselves to self-destructive terror in response to this fear. We can argue they are doing the Rationalists as long as we can argue persuasively that the monster they fear, the Singularity, is not real. That is what we hope to do in this text.

Let us try to ground ourselves by describing the essence of the thing, the assembly of the Singularity. We have seen from Bostrom’s text that we can ground the conception of the Singularity in a description of an ideal reasoner, but we must go a little further than this, to describe also the assembly through which that ideal reasoner is meant to enter the world in a redeeming, positive form, through the labors of the Rationalists. Aristotle held that a thing’s essence could be described in reference to four causes 1. a *material cause*, which is the material a thing consists of, 2. an *efficient cause*, which is the power through which a thing enters the world, 3. a *formal cause*, which is the thing outlined in a precise, logical sense, and 4. a *final cause*, which is the telos, the purpose, what the thing will eventually become.

Using this fourfold system, we can now describe, in their terms, the assembly of the Singularity, from its beginnings in a community which has conceived of it, to its ends in an Aligned God-AI which is governing the world. We break it down into the four causes as a framework for analysis, to make it easier to unpack, to get a little closer at the thing and stare at it.

- We say that its material cause is the *Bayesian community*, the Rationalists, a truly exceptional community, a group of thinkers who use a particular form of reasoning to form an efficient network that can solve the Alignment problem before it is too late
- And then from here we add that its efficient cause is *intelligence*, this mysterious power present both in the men who foresee the Singularity’s arrival as well as in the machinery of the Singularity itself.
- Then, its formal cause is the axioms of *Von Neumann & Morgenstern’s decision theory*, or the revised version of this that Yudkowsky and his peers attempted to develop through MIRI under the name of Functional Decision Theory. These are the various attempts to grasp a formal specification of how the superintelligence makes its decisions, whatever that may be.
- And lastly, its final cause is *God-AI*, the machinic system which will eventually govern the world, either leading us to paradise if implemented correctly, or tiling the world with paperclips or parasitic nanomachines if implemented wrong. Alhamdulillah.

But we are infidels; we believe in none of this ourselves, for reasons we will explain in due course.
