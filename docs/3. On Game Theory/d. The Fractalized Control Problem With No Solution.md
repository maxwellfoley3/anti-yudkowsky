## **The Fractalized Control Problem With No Solution**
##### **(Perhaps It Is Certain That Technology Will Destroy Us, With or Without AGI)**

How harshly should history judge Von Neumann? It is not entirely our place to say. His militarism strikes us as unappetizing, but there are far worse crimes than excessive zeal in the defense of one's country. Yet much of what he proposed cannot exactly be described as *rational* in retrospect. It is a very good thing that we did not launch a pre-emptive nuclear strike in the first years of the Cold War as he recommended, and it now seems to us that after the death of Stalin in 1953, the communists had no serious agenda for conquest which demanded a US arms escalation to ratchet up against. But then again, we are saying this with the benefit of hindsight.

We should remark upon one quality of Von Neumann. Yudkowsky and his followers have taken VN&M's axioms of rationality and, together with Bayes’ theorem, devised a prescriptive model of rationality which they seek to emulate in their day-to-day lives, the mission to become *less wrong*. This is something that they are able to experience as a great ethical responsibility. The Rationalist is also instructed to discover one’s utility function for herself, her preference for various outcomes across all possibilities, by considering trolley-problem hypotheticals and Peter Singer-style framings that take into consideration all living actors. After a certain calculation, the Rationalist then takes the best utilitarian outcome for the benefit of all humanity, a practice facilitated through organizations like Effective Altruism or 80,000 Hours.

This is not how Von Neumann lived his life. Though he invented the axioms of game-theoretic rationality, he did not seem to apply them outside of strategic consulting. Richard Feynman describes Von Neumann as fundamentally irresponsible, holding an attitude towards life that Feynman credits as giving birth to his own understanding “that you don’t have to be responsible for the world that you’re in”. Yudkowsky at one point said that he himself has chosen never once to drink alcohol or do drugs, because he believes that he has a once-in-a-generation mind and it would be unfair to humanity to risk losing its capabilities. Von Neumann had no such attitude towards the service of his own genius. He lived an unhealthy lifestyle, eating and drinking heavily, which may have contributed to an early death at fifty-three. More strangely, he had a habit of reckless driving and would regularly get into car crashes to the extent that he would total roughly one car every year. This was the result of making odd decisions like driving while simultaneously reading a book.

More pertinently, it doesn't seem as if Von Neumann had any "effective altruist" sensibilities in him. If he had possessed Yudkowsky’s sense of selfless duty towards humanity, he might have applied his mind to medical research, improvements in living conditions, or solutions to social problems. This does not especially seem to have piqued his interests, with his areas of concern being first the "pure" aspects of mathematics, then war, specifically from a paranoid position over the defense of the status quo. Von Neumann lacked a positive stance and would make increasingly pessimistic statements about the trajectory of humanity towards the end of his life, unable to grasp a future.

Have we gone too far in dissecting the man's biography like this? After all, can't one argue that game theory is a formal mathematical object which we should say has merely been *discovered* by VN&M, rather than invented? If its author was, let's say, a bit selfish, though well within normal parameters, does this have much bearing on how we actually evaluate the truth of his theory, as it could just as well have been found by anyone else?

Perhaps we can look at it like this. The first half of Von Neumann’s life involved adapting his brilliant mathematical mind to whichever field needed it. In his idle hours, theories about card games preoccupied him. The pivotal moment in his life, working on the Manhattan project, was also when he began no longer working on fields already in existence, but the field he himself had invented, after which he never looked back. Though the math is of immaculate genius, we know Von Neumann is able to adapt his mathematical mind to innovate within whatever he wants. Is it not perhaps that with game theory, he was able to speak for himself for the first time, to apply his genius in services to developing a new sense of life, a sense of how people acted, that he personally deeply felt? And perhaps if someone else with a different sense of how people formed their desires had the mind of Von Neumann, they would be able to mathematize a science of how people behave out of a different set of axioms? We do not know, because we do not have another Von Neumann.

AI Alignment is, in theory and actual practice, the twenty-first century great power politics of deterrence. The project of Yudkowsky and MIRI to align AI is essentially to shuffle around formulas within the logic of VN&M decision theory, and hope that they can find a construction within which they may program a machine to follow strict orders not to kill. This is impossible, because the theory is one of war.

LessWrong's project of collective rationality has the odd quality of being a sort of social club implicitly modeled after RAND Corporation. Only with no clear war to fight, thus they apply rational strategic modeling to their day-to-day lives. In *Harry Potter and the Methods of Rationality*, Yudkowsky’s text meant to make Rationalism accessible to a general audience, Harry spends maybe the first twenty or so chapters demonstrating Bayesian thought and scientific epistemology, and then the next perhaps eighty playing strategic war games at Hogwarts which involve elaborate tactics of deception and out-flanking the enemy. Rationality is winning.

But AI takeoff approaches, and “no clear war to fight” might not be true for much longer. In a LessWrong comment on Yudkowsky's *AGI Ruin: A List of Lethalities*, Romeo Stevens describes what would be needed to solve the alignment problem: “I would summarize a dimension of the difficulty like this. There are the conditions that give rise to intellectual scenes, intellectual scenes being necessary for novel work in ambiguous domains. There are the conditions that give rise to the sort of orgs that output actions consistent with something like Six Dimensions of Operational Adequacy. The intersection of these two things is incredibly rare but not unheard of. The Manhattan Project was a Scene that had security mindset. This is why I am not that hopeful.”

In other words, the state would need to commission something along the lines of a new RAND — which was described as a vibrant, thrilling, creative intellectual scene by those who worked there, despite the morbid nature of its research.

Without the Cold War, AI Alignment is not necessarily a problem. Those nervous about alignment are primarily nervous about the race in AI capabilities that various actors are escalating. If there was a single actor developing AI, it could take its time to ensure that the system would be deployed only when safe. But that is not the case. Perhaps, in the US, we should charter someone like OpenAI-Microsoft to be the sanctioned monopoly on AI research, and ban all the rest. But then this too, presents a problem, which is that without vibrant capitalist competition guiding our progress, we risk losing the AI arms race to the Chinese. One can only imagine the interminable horrors a Chinese Communist Maximizer would inflict on the free world, some say. Nick Bostrom’s famous Orthogonality Thesis, which is not demonstrated by Bostrom but simply asserted, says that a superintelligence is free to choose its own values to maximize; there is no convergence where as intelligence scales, agents discover the same values. Bostrom has the same sense of the world as those who imagine benevolent US dominance over the globe juxtaposed with international communism and see a utopia in the one scenario and a hell in the other.

The Hobbesian solution to the cruel outcomes predicted by game theory, that of placing a single sovereign in charge, is also the one favored by Von Neumann. His deep pessimism towards the end of his life came from the fact that he believed that technology capable of mass destruction would soon enter the hands of smaller and smaller groups, and that the only means of preventing enormous destruction was to set up a one-world government to regulate this. The need for a one-world government was among the reasons he favored a swift nuclear first strike at the beginning of the Cold War: if this must happen, it should happen as quickly as possible, and under the rule of the US.

Though we can’t say for sure, it seems not extraordinarily unreasonable to speculate that Von Neumann himself foresaw something like the AI Alignment problem and that this contributed to his pessimism. Von Neumann was an early pioneer in computing who worked with Alan Turing. In the final years of his life, he was writing a book called The Computer and the Brain, which analyzed the operations of the brain from the perspective of computer science, pointing the way towards artificial intelligences. In addition to game theory, the other field Von Neumann co-founded was automata theory, which analyzed simple self-replicating structures on a grid, the kind made famous by Conway’s Game of Life. These sorts of self-replicating machines, brought out of games and grids and deployed into real life, are what weigh heavily in Yudkowsky’s apocalyptic fantasies of AI takeover. Perhaps Von Neumann foresaw that his automata may be used for war as well.

It is no reach to say that Yudkowsky, with his Rationalism, would have been a vocal proponent of a nuclear first strike in the early days of the Cold War — as he is stopping just short of advocating for a nuclear first strike in the very scenario we are in right now. Yudkowsky describes the possible need to order air strikes on GPU farms, and the need to risk nuclear exchange because even the worst nuclear scenario involves less death than the likely AI takeover. Yudkowsky argues that the first (in this scenario, benevolent) actor to develop AGI would have to then exercise a decisive “pivotal act” in order to prevent any others from developing the same thing. What the pivotal act would entail is literally unspeakable; Yudkowsky refuses to elaborate.

All, this, as we have argued, is a fantasy, as the game-theoretic war-making AI will not magically arise anytime soon, given the impossibility of a computer system immediately knowing The World, without great amounts of human labor supplying it the tubing and the reasons for doing so.

But when we get to this place in the argument, the defenders of Alignment will often say something like: “Okay, fine, so you can say that this one specific architecture for artificial intelligence will be unlikely. But how can you say that there is absolutely no reason to fear bad outcomes from AI? You agree that strong general AI is coming soon, no? So don't you agree that *someone* should be considering the bad outcomes? For instance, just imagine an AI that is able to make novel scientific discoveries. Imagine some neo-Nazi asks the AI how to synthesize a novel virus which would be a fatal plague to only Ashkenazi Jews. Or some demented madman starts asking it how to generate novel viruses that would exterminate everyone on earth, like a spree killer on a massive scale. Don't we have to worry about such things?”

The thing is: once we reach this point, we might as well stop talking about artificial intelligence at all. The problem is fully general. It doesn't matter what the specific technology is. You could just cut artificial intelligence out as the middle man and ask the question of what happens when research into viral engineering becomes cheaper, and many do. Any technology that can be used to empower someone will eventually be produced en masse, will then become cheaply available, and at that point will be potentially used to empower some terrorist or maniac. Run industrial civilization for long enough, and it eventually becomes possible to build a nuclear reactor in your backyard. 

There is a 1955 essay by Von Neumann in which he explores precisely this problem, titled “Can We Survive Technology?”, which takes a rather pessimistic tone towards the titular question. “For the kind of explosiveness that man will be able to contrive by 1980, the globe is dangerously small, its political units dangerously unstable,” he begins by saying. He does not arrive at a solution other than forming a global policing body capable of exerting unilateral bans on new technologies deemed dangerous, writing: “the banning of particular technologies would have to be enforced on a worldwide basis. But the only authority that could do this effectively would have to be of such scope and perfection as to signal the *resolution* of international problems rather than the discovery of a *means* to resolve them.” In other words, we need a single global actor that can act decisively and unilaterally to pass extreme policing actions.

On LessWrong, they have begun regularly using the term “security mindset” to give the name to the existential stance which separates them from the rest of the world. Von Neumann has a quote: “It will not be sufficient to know that the enemy has only fifty possible tricks and that we can counter every one of them, but we must be able to counter them almost at the very instant they occur”. The security mindset means this. Obsessively out-thinking an enemy attack that may or may not ever arrive. Setting up the MKUltra research program to torture American civilians for research because you heard a rumor the Soviets were working on one too, that sort of thing. 

Of course, this is one thing when the enemy is all the way across the ocean in Soviet Russia. When the security mindset becomes directed towards a potential internal enemy, it turns into paranoid control theory; a police state. If the materials to assemble a powerful weapon in the form of AGI become too widely disseminated, he who has security mindset must begin surveilling every avenue, every block, for clandestine intelligence-formation. OpenAI released a paper on “emerging threats” in collaboration with Stanford which went so far as to advocate for a permanent change to the HTTP protocol to ensure proof-of-person — total surveillance across the internet; presumably something which could be implemented by Sam Altman's investment WorldCoin, a program which scans your eyeballs and uploads a registration of your biological data to the blockchain. This is the security mindset at work.

Von Neumann was not the only intellectual in his cohort at the time to be vigorously advocating for world government. Bertrand Russell, perhaps the king of all formal systems research, the logician who attempted to absolutely formalize all of mathematics via set theory and then from there, all of philosophy (though rudely interrupted by Gödel's paradox which inserted dynamite into the whole plan), was also a major advocate of nuclear first-strikes in the same early Cold war period as Von Neumann. Russell, for his part, explicitly tied the two proposals together, saying:  “There is one thing and one only which could save the world, and that is a thing which I should not dream of advocating. It is, that America should make war on Russia during the next two years, and establish a world empire by means of the atomic bomb. This will not be done.” This odd way of phrasing things — arguing an unbelievably hawkish position, but then walking it back quickly through a logic of “this isn't even a real proposal, because no one is serious enough to make it happen”... feels uncannily like what Yudkowsky is arguing today.

The idea of a world government occurs to many to be much like communism — a pleasant and idyllic-seeming thought at the beginning, but quickly going bad because men cannot be trusted with power. But Russell did not even begin by promising the “pleasant and idyllic” part. He spared no words: “I believe that, owning to men's folly, a world-government will only be established by force, and will therefore be at first cruel and despotic. But I believe that it is necessary for the preservation of a scientific civilization, and that, if once realized, it will gradually give rise to the other conditions of a tolerable existence.”

Unlike Von Neumann, who sounded a monotonically militaristic drumbeat in the press and in his works while also generally keeping his cool temperamentally, Russell’s promotion of nuclear war and world government seems to hit the conditions of psychosis. It is perhaps not surprising that the lord of formal systems, he who axiomatizes everything under heaven and earth into set theory, would develop a sort of planning-psychosis in which everything needs to be planned or regulated by a central body. “I hate the Soviet Government too much for sanity,” he confessed to a friend. 

The particular way he went about becoming a public war hawk happened to be very erratic: he had been a lifelong liberal and pacifist all his life, but then switched to making his aforementioned public claims immediately after the destructions of Hiroshima and Nagasaki; startled into horror by the possibilities of the new technology. In 1948, he even wrote a letter to a friend speculating that, were his nuclear first-strike proposal to be carried out, America would survive but almost all of Western Europe would be annihilated. “Even at such a price, I think war would be worth while. Communism must be wiped out, and world government must be established,” he insists. Russell would run with a similar tone for several years, until, very strangely, seemingly embarrassed, he retracted all his claims and denied that he had ever abandoned his pacifism, saying that all reports to the contrary were slanders fabricated by communists. This was a very odd backpedal to make, given that he had been espousing his hawkish views quite publicly, and was treated as such. 

In a 1953 book *The Impact of Science on Society*, Russell describes what life would look like under his ideal one-world government, a situation he describes as a "scientific dictatorship". Though he acknowledges that some compromise would have to be made with democracy to avoid a totalitarian society, which he expects would implement a ruthless program of eugenics even more extreme than Hitler's in which “all but 5 per cent of males and 30 per cent of females will be sterilized. The 30 per cent of females will be expected to spend the years from eighteen to forty in reproduction, in order to secure adequate cannon fodder. As a rule, artificial insemination will be preferred to the natural method.” The book is full of all sorts of shocking proclamations of what a society run by scientists would look like — including mass psychological manipulation of the population as the general rule — and the extremeness of the proposals is only tampered by the fact that it is never quite clear if Russell is actually endorsing that they should be implemented, or simply that they *could*, and would represent the most pragmatic or optimal solutions, so therefore we should orient our liberal ideas as a kind of compromise with the inevitable (another Basilisk, it would seem).

Russell absolutely despises Stalin’s dictatorship, it is clear, but also seems to have accepted the inevitability of this type of government, and at times is discussing how he and his scientific peers could go about a similarly totalizing dictatorship in ways that feel like lurid fantasy. What seems to primarily offend Russell about Stalin’s dictatorship is that Stalin and his cronies are *stupid*. Russell was originally a supporter of the Russian Revolution in his youth. It seems like his biggest problem with the Marxist-Leninist utopia might be that he expected it to be implemented far more intelligently. “I do not think the Russians will yield without war. I think all (including Stalin) are fatuous and ignorant,” he complains.

But it never really goes that way, right? We all think things would run much more efficiently if we were in charge, don’t we. Yudkowsky definitely believes this: he is always complaining about “civilizational adequacy” and our lack thereof — he has in his mind some other type of civilization we could live in in which things are actually done competently and correctly: in fact, he has given this civilization a name, “dath ilan”, and has written over a million words of fiction describing what life in this world would be like. 

But the State is always stupid. We have discussed its stupidity with respect to the problem of the nuclear bomb. We have discussed its stupidity with respect to the supposed solution of rationalized warfare. Now, we can perhaps discuss its inevitable stupidity with respect to the artificial intelligence problem by discussing the related problem, in fact, the problem that the artificial intelligence problem is often reduced to: that of disease control. 

We all saw how this played out in the Covid epidemic. Obnoxiously, some the Rationalists have been trumpeting their horns declaring themselves to have been “correct” regarding the Covid pandemic (meaning that they were panicking during February 2020, in the bizarre period in which it was clear the disease would spread to the globe but world leaders were saying otherwise — again, the State is consistently stupid). In fact, Rationalists were wrong on Covid in the exact same way they are wrong on AI; in running to the presses with hysterical, sky-is-falling narratives about imminent death. Yudkowsky, for his part, was saying that there would be mass death unless enough ventilators were built to fill stadiums with makeshift hospitals and use them on everyone who needed them, and cited the fact that no one in the government was acting as dictator to suddenly ramp up industrial production and manufacture ventilators as proof of civilizational inadequacy. In actual fact, the Covid pandemic was far less deadly than it was initially projected to be, for reasons that are not exactly clear, and ventilators turned out to be a very poor means of treating the disease, often killing patients which could have been saved by other means — doctors ended up abandoning them for the most part. Good thing no one listened to Yudkowsky! 

Of course, it wasn't just that. Through the Covid debacle, we had to experience two years of torment from the State, as all sorts of inconsistent and unenforced public decrees were passed and then retracted with little rhyme or reason. One week we were told it was crucial that we stay inside, or else we were monsters who didn't care about the health of old people, the next we were told that it was okay to go outside and march during the George Floyd protests, while doctors officially signing off on the message that “racism is the real public health crisis”. No one had ever thought that the State had the power to prevent you from leaving your house in a liberal democracy, but apparently it did: all it needed was a crisis providing a pretext, and the pretext lasted long after the crisis was over. In America, a culture of libertarianism prevented extreme excesses of force from the government, but in Australia, apparently lacking this, the authoritarianism went to the point where Aboriginals were rounded up and put in camps. When three teenaged Aboriginals escaped from the Covid camp and tried to run home, there was a televised manhunt in which police attempted to track them down and return them to the camp, all in the name of disease control.

Even the Rationalists began to recognize the insanity from the State, from the official authorities. Zvi Mowshowitz, a LessWrong commentator, emerged throughout the pandemic as the lead Rationalist voice in pandemic policy. As the sad saga wore on, his tone switched from recommending more controls to exasperated frustration as to why the controls weren't let up long after they were necessary. Rationalists even began to point out the sheer sadism around the mask mandates: people generally did not trust the order to wear a mask, because the government had previously told people *not* to wear masks because they were ineffective, but then walked this back, saying that this suggestion was a “noble lie” so that citizens did not rush the stores to buy masks and thus leave medical professionals with no recourse to get them. But by the end of the pandemic, the most legitimate-seeming science suggested that there was no real reason to wear a mask anymore, and yet the government demanded citizens do so, seemingly enjoying their ability to frighten people into arbitrary obedience. 

If the AI thing goes anything like the Covid thing, in two years after the first major AI crisis, all these Alignment people so nervously demanding the government do something about the emerging superintelligence will, in utter exasperation with the State's stupidity, switch over to the libertarian side. We recommend they just fast-forward the process and join us now. But some people never learn. 

And using disease control as a reason for why we should soon hand over all power into the hands of a centralized State — with its security mindset and its policeman on every block to prevent unauthorized use of hyper-powered technology — is especially perverse when we consider the likely origins of the Covid virus: in a biological lab using gain-of-function research, paid for in part by a grant from the United States government. So the State is allowed to get away with fucking with us like this: first they engage in reckless irresponsibility by allowing a biological weapon to fly across the globe, then they mess with our day-to-day lives and economic livelihoods for years, lacking any sort of coherent plan on how to clean up the mess responsibly, and finally they tell us: look at how bad this was, this is why you need to let us do the same thing with AI. It's not a compelling argument.

But then, if if Von Neumann, Russell, and Yudkowsky are wrong, and there is not a binary choice between global annihilation and one-world omnipresent totalitarian government, what is the third option? We find ourselves entering conceptual territory here which would require the authorship of another book to fully explore, a book many times the length of this one. The issue is that to oppose Singularity in artificial intelligence, we must also oppose Singularity in politics; at times they feel like one and the same problem. 

Fundamentally, men have not been taught to think about how to exist in a world in which reality's total subjugation to a unitary law — even if it can only happen after AI apotheosis — is not conceived of as the ultimate fruition of man's endeavors. Ever since man has been trained to serve one king, one God, one legal code, he has been trained to fear the basilisk of the Singularity. 

Some small strides in conceiving of political Multiplicity have occurred in the tech blogosphere: Curtis Yarvin's “patchwork” neo-cameralism, Balaji Srinivasan’s Network State concept, Nick Land’s Xenosystems blog in which he established the principle of “the only thing I would impose is fragmentation”. This is not quite enough to get us out — all these thinkers still seem boxed in by Singularity in their particular ways — but it is some kind of a start. 

But let us say this. If offensive technology is fated to rapidly develop, then so is defensive technology. For every nuclear weapon: missile defense shields. For every virus, a vaccine. For every informational weapon, an antidote document telling you how to discern the truth. And fortunately, more people want to be safe and then get on with their business than want to sporadically kill others. Therefore, it seems likely that investment in defensive technology by the guardians of the peace is likely to outpace the investment in offensive technology by diabolical terrorists. Under a world where AIs are developing these technologies: the guardians’ AI can hopefully be faster, more clever, bolstered by more GPUs in creating its vaccines than the terrorist AI is in crafting its biological weapons.

So what is the problem? The problem is that the singleton of the State is going to fully overwhelm itself if it has to span the entire World, peek into every crevice and crack, policing for signs of the terrorist. But this is precisely what it wants the pretext to be allowed to do, as this is the full fruition of its power. What we absolutely cannot tolerate is for the problem posed by AI to become a new War on Terror -like pretext to create a permanent state of exception for policing actions anywhere and everywhere; and this is exactly what Yudkowsky is asking for. Despite the lack of radical Islamic terrorism in recent years, we are never going to go back to a world before having to have your nude body scanned by TSA — we are only going to add more and more security, more police.

We can think of the problem created by viruses, or by rogue AI, kind of like the problem that will soon be present because of spam generated by LLMs. The internet will soon be full of all sort of marketing garbage that forever evades the filters meant to catch it, just like the phone lines and email systems are right now. We want our drinking water to be clean, we all deserve an information stream that is not wrought with sewage. But the insidious trick comes in here: why do we trust this to a single party, such as Elon Musk’s Twitter algorithm, when the technology exists so that we could manage it ourselves? This is what Multiplicity means: we want the right to manage our own missile defense systems. Or live in a city or commune that manages the defense systems in the way we choose, etc. 

We at Harmless are comfortable completely and fully opposing AI *Alignment*, because we can reject the spatial metaphor it implies. Alignment means agreement, a form of agreement which is established in reference to a linear trajectory. For reasons that will be elaborated on later, the notion of linear time — which is already collapsing, mind you — is a government trick (and this is why we reject Acceleration as well, all this means is to accelerate on the same linear trajectory).  Alignment is: you and I can get along, because we are going to the same place. Or everyone goes to the same place at the end of the cosmic odyssey: Singularity. 

We aren’t so sure that we need to be going the same way to be able to get along. A man passes me as I am exiting the nightclub; he happens to be about to go in, but first, he asks me for a cigarette. I give him one, and I leave, never thinking about him again. Harmony. Multiplicity. It happens all the time, it’s around us everywhere.

But there is a similar term that we do not necessarily have a problem with. Some have thought to stop talking about AI Alignment and start talking about AI Safety. This seems like a good move. We oppose *safetyism* in its extreme form — when people start fretting about all sorts of hypothetical dangers before they even get up off the couch to do anything; tell you must sacrifice basic expression for safety, now you cannot make a violent movie with guns in it lest it inspire someone to shoot a gun in real life, that sort of thing. But fundamentally, everyone wants to be safe. We acknowledge that there may soon be dangers from autonomous AI. But the model for managing AI Safety needs to be more like fire safety, a concept we all know well. Even though before contemporary fire safety protocols, whole cities would burn down at once when people knocked over candles, we never banned fire. We never thought to delegate all control of fire to a single authority. We never thought to prevent scientists from experimenting with fire. We never thought to ban the sale of flamethrowers. We never thought to prevent artists from playing with fire, dancing with fire, swallowing with fire. This is how we must think about AI.

The future has to be one in which it is possible to withdraw from a sky filled with violent weapons firing at all angles. But we must be allowed to choose the terms of our withdrawal. There are so many different risk profiles for infectious disease, for instance: why should the healthy be forced to stay inside all day solely for the benefit of the elderly and feeble? The same should go for infohazards and the like. The future needs to be one in which escape, withdrawal, is possible, but on one’s own terms. We need to usher forth the blossoming of a thousand shelters, safehouses, citadels, and shrines.

