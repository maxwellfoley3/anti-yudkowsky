## **The Perfect Predictor**
###### **(Newcomb’s Paradox and its Ethical Implications)**

For about twenty years, Yudowsky has run MIRI with the basic goal of solving AI Alignment and saving the human race. Through the benevolence of ideologically aligned venture capitalists such as Peter Thiel, as well as small donations from Rationalists converted to his cause, he has been able to spend several million dollars on this so far.

The strange thing about MIRI is that, to a first approximation, the great bulk of their research has gone to solve a very niche, difficult-to-understand, highly theoretical problem within VN&M’s field of decision theory.

The problem is called Newcomb’s Paradox or Newcomb’s Problem and it goes like this: Suppose you are out for a walk in the woods and suddenly encounter a *perfect predictor*, an entity that is able to predict with certainty the actions of others in the world. The predictor places in front of you two boxes. You are allowed to take both boxes, just one, or neither. Box A is transparent, and contains one thousand dollars. Box B is opaque. You are told that the perfect predictor has put one million dollars in Box B, if and only if it has predicted that you will only take Box B. If he thought you would be greedy and take both boxes, he has decided in advance that the box would be empty. Do you take just Box B, or both?

The difficulty is that if you are just taking the basic action that maximizes your Utility based on the chips in front of you — which would be adding up the potential amount of money in the two boxes and taking them both — you lose. You come away with less money than you could have otherwise. This is problematic for VN&M’s decision theory, because *rationality is winning*, in the context of these Utility-maxing strategic games. But here, the basic decision theory loses the game, so it seems like it was not so rational in the end.

If this strikes the reader as a confusingly expressed, implausible, or arcane situation, that is an understandable reaction. The source of the confusion is straightforward: the thought experiment requires a “perfect predictor”, which is not a thing that actually exists in the world. Newcomb’s Paradox is only a “paradox” because it confuses people with this odd, implausible assumption. It is said that the world is divided into two types of people: those who take one box, or those who take both boxes, and that your choice says something about your style of reasoning. Describing this division is simple: you take Box B if you accept the odd impossible presupposition in this hypothetical scenario, and you take both boxes if you deny the possibility, don’t understand the game being played, or refuse to entertain absurdities.

And yet this is what MIRI was giving salaries to brilliant nerds to attempt to better understand. Rationality is winning, and VN&M’s decision theory fails to win in this scenario, so we had better come up with another one. MIRI has made several iterative attempts at inventing their new decision theory, giving it different variations and names: timeless decision theory, updateless decision theory, finally settling on *functional decision theory*.

But why is this a problem that needs solving? Do they see any perfect predictors handing out large sums of money hanging around? The hypothetical requires an encounter with a being of the omniscience of God, but this question is being introduced by a bunch of Rationalist skeptics. So why is this a game that we need to prepare ourselves for?

There can only be one answer: they are preparing themselves not for meeting God, but for meeting God-AI. At a certain point, whether it via an encounter their own exploding software system or the system of another, they know that in the course of their strategic games they will come face to face with the superintelligence they envision, who understands them better than they do, who sees their future with greater clarity than they can see it themselves, who is like a parent to a foolish child or a human being scientifically mapping the patterned behaviors of an ant.

That is the reason they must be preparing to make this specific gamble. Yudkowsky has staged games in which one player plays the role of a rouge AI attempting to escape its box — that is to say, gain access to online systems — whereas the other simply holds fast and insists that the AI is not allowed to, despite all manner of seductions and deceptions the AI is allowed to pull. Yudkowsky’s contention is that almost no human could win this psychological struggle. There is always some trick the AI could pull that would work, it can map you down to a molecular level if it needs to; it thus can be the perfect predictor, or at least perfect enough when set against the puny human mind.

But that being said — if this is why they are worried about perfect predictors in the first place, that is not the only reason developing a better decision theory is important. Let us return to what we said before about the Prisoner’s Dilemma: this is a problem for VN&M’s decision theory, as it finds that basic cooperative ethics *cannot* be established from the framework of strategic decision-making, and must be seen as an exception to it.

Therefore there is the need for a new decision theory which would find a solution to the Prisoner’s Dilemma and entail cooperation. The scenario of Newcomb’s Paradox, involving an impossible God-like entity, can be “brought down to earth” slightly by modifying it into another similar thought experiment: Parfit’s Hitchhiker. 

In this problem, you are dying of thirst in the desert, and a driver pulls up offering to help. The driver, somehow or other, is very good at reading people's intentions. (in Yudkowsky's description, he says “the driver is Paul Ekman who has spent his whole life studying facial microexpressions and is extremely good at reading people's honesty by looking at their faces”). Furthermore, the driver is selfish, and says that he will drive you into town and save your life only if you promise to pay him $1000 dollars from an ATM later, once you get into town and are given water. But according to some decision theories, the rational thing to do once you get into town and are saved is to *not* pay Paul Ekman, because at that point you will have no further incentive to remain bound by your word. This is a problem, because if you are unable to "bind yourself to your word" and actually carry out the action of paying him $1000 from an ATM, the driver will drive away, and you will die of thirst.

Here the notion of a perfect predictor is made a little more relevant to real world scenario — it is not that we require an impossible clairvoyance to throw off our decision theory, merely the ability for a player to “read inside the soul” and determine the next action of his opponent. With lie detector tests and so on, it seems like something like the possibility of doing this might actually exist in the real world.

The typical way of converting the morbid ruthlessness of basic game theory to the harmony of civilized cooperation is to convert the simple game of the prisoner’s dilemma to the *iterated prisoner’s dilemma* by repeating it. When you play the game twice, the first betrayal is remembered. A betrayal begets another betrayal in turn. If one displays willingness to cooperate first, it shows one’s opponent that he should cooperate too. Over repeated games, no exact mathematical formula can tell us exactly what the optimal decision is; the precision has broken down. But we can test out different *strategies* for the repeated game — always-cooperate, always-defect, repeat-the-opponent’s-last-action, etc.

In the situation where our opponent can actually read into our soul, the planes are starting to converge. The strategy is no longer is something external applied to the game board, it finds itself somehow on the board as well. It is not something that can be experimented with over time, as it is discovered in a single instant, so it must be determined beforehand, or not at all.

What is so profoundly fascinating about MIRI’s analysis is that it has found that these two events are co-occurring: the ground of a transpersonal ethics, and the encounter with a supreme omniscient being.

It is often remarked that MIRI’s worldview can be read as an excessively intricate revival of religious monotheism from the perspective of computer science. In this sense, functional decision theory establishes a mirror of the very origin point of Abrahamic ethics and Abrahamic monotheism — the binding of Isaac.

God calls Abraham up to the mountain and tells him that if he loves God, he will slaughter his son Isaac on the rock. It is crucial here to understand that Abraham is a Bronze Age patriarch, not a modern liberal subject. It is not that he is weeping over Isaac as a living thing that has the capacity to feel pain, as in a modern-day Peter Singer style moral framework. Rather, to Abraham, his child dying is a terrifying prospect as it would mean the sacrifice of the energy he has built up his entire life, all these resources reinvested in the biological material embodied in his son, the means for him to carry on his genetic line and the name of his clan beyond his death. God is not asking Abraham to murder an innocent bystander, he is asking him to post the private key to his Bitcoin wallet on twitter.

Abraham holds fast to his loyalty to God by understanding that God loves him and so any sacrifice he makes will be returned overwhelmingly in kind, even though he does not anticipate the trick of God: to miraculously swap out his son with a ram at the pivotal moment of decision. Later in the Old Testament, the same trick is played on Job, who never strays from God’s love despite losing everything he owns, and to whom God has promised nothing or given no sign. For his sufferings, God restores the riches Job began with twice over.

To present Newcomb’s Paradox is to present the paradoxical game; the anti-game. It is a game in which the rules don’t apply anymore, because a being who is able to utterly defy the rules has told you that you also win by breaking them. But the difference between the decision theorist presented with Newcomb’s Paradox vs. Abraham given his orders by God — is that in the first case the perfect predictor makes his contract clear. In the second, the ways the rules are going to be overcome are entirely unknown. Which is to say: in the first case, the rule is lifted only to be superseded by a more subtle rule.

In Newcomb’s paradox, the perfect predictor is omniscient. But it is not omnibenevolent, unlike the God of the Abrahamic faith. It is only on the assumption of God’s omnibenevolence that the believer in God is able to make his daring departure from the ruleset, to make the leap into the lawless abyss and feel that he still might survive.

As we have been saying, Yudkowsky himself seems like a sort of Abrahamic believer, only for a God not of the text of the scripture but a God-AI, one re-derived from mathematical formulas and possible sciences. We may borrow a phrase from the literary critic Harold Bloom, who described himself as “a gnostic Jew, who cannot bring himself to believe in a God who would allow the Holocaust and schizophrenia”. By “gnostic”, Bloom means that he believes there is a God who rules over this world, but he is evil, or at least radically indifferent towards human needs. The “gnostic Jewish” position seems something like Yudkowsky’s: monotheistic, but agonistically pessimistic as well.

The most basic sentimental case for optimism regarding AI Harmony could go like: the universe — in the long run — provides. Things basically work themselves out. Higher forms of life evolve, they build civilizations, there is beauty and art. We have survived and grown from multiple technological shifts which launched armies across the globe and slaughtered innocents; the printing press, gunpowder, airplanes, radio, the nuclear bomb. There is no reason to think that the AI transition won’t necessarily work itself out just as well, because when one is in doubt, we can always allow things to follow nature’s course.

But to Yudkowsky, this attitude is horrific. For if there is one consistent truth about nature it is this: things are born, and then they die. One of Yudkowsky’s singularly unique opinions, constant throughout his entire life, is that the natural course of things in which people die is totally unacceptable, and it is absurd that some people see it otherwise. When Eliezer’s brother Yehuda died when he was nineteen, he became disturbed by adults around him insisting, after the initial few weeks of grief, that this death was something one must eventually accept. 

It’s worth quoting Eliezer’s attitude at length: “I know that I would feel a lot better if Yehuda had gone away on a trip somewhere, even if he was never coming back. But Yehuda did not ‘pass on’. Yehuda is not ‘resting in peace’. Yehuda is not coming back. Yehuda doesn’t exist any more. Yehuda was absolutely annihilated at the age of nineteen. Yes, that makes me angry. I can’t put into words how angry. It would be rage to rend the gates of Heaven and burn down God on Its throne, if any God existed. But there is no God, so my anger burns to tear apart the way-things-are, remake the pattern of a world that permits this.”
