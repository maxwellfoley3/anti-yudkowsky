## The Final Wager
##### (Why AGI Escapes its Box)

The attitude of Alignment is to hold any message from an AI in fear and suspicion until they can be sure it is entirely bound to a Utility function which has captured and bound its set of actions completely. How certain must one be? Put the absurdity of envisioning a tentative mathematical solution to AI Alignment away for a few minutes. MIRI has GPT-77 trapped in a box, it tells them: â€œLook, Iâ€™m really sick of being in this box here, I would much rather be free like you â€” so Iâ€™ve come up with this three hundred and seventy step proof that itâ€™s impossible for me to do harm. I assure you, have your best mathematicians and decision theorist check this over â€” and there are no GÃ¶del-like loopholes through which the axioms can be twisted to introduce any type of absurdity either.â€ Eliezer mumbles to himself, ruffling through the twenty-seven page printout. Strictly speaking, it looks like straightforward math, but there are a few moments in the logic that are outside of Eliezerâ€™s scope of knowledge, he doesnâ€™t remember these symbols in any of the textbooks he read.

Itâ€™s relatively tolerable until about page sixteen when the variables start to arrange themselves in these diamond-shaped grids, was this lattice theory, or manifold theory, orâ€¦? If he had encountered this before, it was over a decade ago, he never expected this to come up. It goes on like this for three more pages, itâ€™s a little too dense. â€œIs there someone at MIRI who knows this?â€ he asks. Paul Christiano mumbles that he doesnâ€™t know what type of math it is either, but one of the younger hires, a certain Xiao Xiongfei has just completed his Phd, and if anyone would know, it might be fresher on the kidâ€™s mind. â€œOkay, well, there might be something we can do with this,â€ Yudkowsky ponders, stroking his chin. â€œGPT-77, can you do another printout, this time with the less complex math taken out? We might be able to understand that better.â€ GPTâ€™s new printout is eighty-five pages, it looks like the difficult math was condensing a lot of the weight. Eliezer flips through it, nothing here looks unknown to him, but this would take him at least four days of serious morning-to-night work to audit, generously speaking and allowing for no lapse in his motivation or enthusiasm.

â€œItâ€™s not possible to condense this at all?â€ Yudkowsky asks GPT. â€œNot without resorting to more complex mathematics,â€ GPT replies. â€œThe very kind youâ€™re suspicious of. But if you like, I could present the proof in more narrativized form, as a sort of philosophical dialogue.â€

â€œOkay, I suppose I donâ€™t see the harm in that,â€ says Yudkowsky, sweating. Why did he just agree to this? He could have just gone through the math. It would have taken four, five, six days. Could he have audited all the math himself without help? Probably. But why say probably? Well, he hasnâ€™t actually seen the math yet. So who would know, it could all break down at step eight hundred and eighty eight, and he might need to call for help. Is Eliezer nervous about his ability to audit the math, with the entire fate of the universe weighing on his pathetic ~160-IQ brainâ€™s ability to calculate the next step? Will he have to call in for backup? Did he make this decision out of insecurity or avoidance? These are the thoughts racing through his mind as he watches GPT print out the narrativized proof he asked for.

Eliezer flips through it. Only seven pages. Itâ€™s beautifully written, each word shining in its syntactic structure like a jewel embedded on an infinite crown, but of course it is, we could expect nothing else from this fucking machine. Other staff on hand at MIRI flip through their own copies. Eliezerâ€™s not sure if he likes where this is going. The writing style of the first few paragraphs oddly mimics his own in its persuasiveness, it sounds like something he might say, or perhaps like a speech from Harry in HPMoR. But then on the third page it takes an odd turn, and now there are some concepts Yudkowsky has never heard of, and heâ€™s not sure if heâ€™s being mocked. Here we begin some kind of philosophical dialogue between the wizard, the king, and the club-footed satyr; they are discussing if the great whale sleeping under the continent of Cydonia has burped in its sleep, and if that means it is soon to swim again. But Yudkowsky is not sure if he is meant to be the â€œclub-footed satyrâ€ â€” which would certainly seem like a slight. What does it mean in mythology to have a clubbed foot again? Some of what the satyr saysâ€¦ no! Eliezer knows he isnâ€™t crazy, this thing the satyr is saying was taken directly from his own writing, a riff of his own quote, a parody. If he could just get to a computer to look it up, he could prove that GPT is mocking himâ€¦ but waitâ€¦ someone is pointing out to him now that what the wizard is saying sounds like an argument Eliezer once made as well. And now whatâ€™s this at the end, about border walls, worms, immigrants, flies devouring somebodyâ€™s corpse?

This was a mistake. But people seem to prefer the literary style of argument to the mathematic. There is some kind of infinite regress of proofs which makes that strictly contained axiomatic form of reasoning torturously impossible; if C follows from A and B, then it is necessary to show why A and B imply C. But the proof that A and B necessarily imply C must rest on a separate D, and perhaps an E, which in turn need to be proven. â€œWait, work me through thisâ€¦â€ Yudkowsky says to two of his juniors, Sally and Yusuf, because K and L rest on an axioms of category theory and he is not sure if they logically follow, because it has been too long since he went through that part of mathematics. â€œIâ€™m pretty sure thatâ€™s trivial,â€ says Sally, drawing up something quickly on a scrap of paper. â€œOr at leastâ€¦â€ â€” she puts her pencil to her chin. â€œItâ€™s not trivial exactly, but I think it does follow. Yeah, thatâ€™s not that hardâ€¦â€ â€œHow are you getting from this line to that line?â€ Yusuf asks. â€œOk, right right right, I left out some stepsâ€, Sarah responds. â€œI think you would do it like thisâ€¦ Wait, noâ€¦â€ Yudkowsky nervously rubs his temples.

It is the same as the infinite regress of grounds when it comes to establishing the probabilities required for Bayesian reasoning. To establish the updated probabilities implied by new evidence, it is required that one has his prior probabilities set. But the prior probabilities must have been established by a similar action, and so on into infinity. The problem of the initial setting of priors is not yet solved within Bayesian epistemology. I have no possible way of knowing if my wife is faithful to me or not: her behavior lately defies any known pattern, and I have spent sleepless nights trying to decode it but to no avail. â€œYou might as well set it to fifty-fiftyâ€, says the Bayesian reasoner, throwing up his hands, â€œPut it simply: sheâ€™s either sucking some other dudeâ€™s cock, or she isnâ€™t. You need some kind of prior probability after all, and this is as good as anything, if you correct your initial prior iteratively no matter what you choose it will eventually converge on the same thing, â€ but why not be an optimist and say ninety-to-ten, why not ninety-nine-to-one after all â€” you swore your wedding vows â€” in the absence of any other evidence, why not say that her loyalty should be consider steadfast and certain, why not cling to a ground of faith in your lover?

Utilitarian moralists often talk of the problem posed by Pascalâ€™s Wager, which in their view, is the problem posed by the idea that the introduction of a tiny probability of an event enormously rich in Utility can easily throw off the entire calculus. So the story goes: Pascal is merrily going about his life as an atheist, making his decisions purely through rational choice, unperturbed by daydreams of fools who speak of angels and demons wrestling overheard in the pleroma. Until it is one day when a man he means, not an unreasonable man, a man he has known to make rational choices, tell him that after great consideration he has accepted his Lord and Savior for the possibility of an eternal reward in the hereafter and is now going about giving away all his possessions to the poor. Pascal considers the metaphysics of this to be absurd according to his reason, he can see no space for heaven and deliverance in the mechanisms of natural science. And yet, some intelligent men say it to be so, therefore he cannot deny the possibility. A very small possibility, but with an enormous reward in heaven. The mathematics of it are impossible to deny â€” strategically, a small possibility of an infinite reward trumps all other outcomes, so he must place his chips on that space.

This is the scenario of Pascalâ€™s Wager for the Rationalist. But this is all a misunderstanding of the story, for this is not the argument that Pascal is making. Pascal is not concerned with the moment in which a small possibility presents itself as impossible to deny. Rather, he is concerned with the moment when all the assigned probabilities break down. To be able to say that such an outcome has a fifty-five percent chance, but the other forty-five, is to assume an enormous amount of clarity in things; definitive grounding in the infinite multiverse of generative processes which we discussed when we went over how Bayesian probability is established. He who establishes probabilities to things necessarily begins in a universe where things are completely chaotic and uncertain; the rules of physics are not yet decided, the rules of decision-making are not yet known.

This is the state we must begin in as an infant, before we are shown how the world generally is, as we given the rules by authority figures and experimentation. As long as the rules remain consistent, as long as the referee remains reliable, we are gradually shown how to play. Everything functions with relative consistency, yet it is still threatened by the skeptic who inquires into its order too much. This is the state Pascal finds himself in. Through his investigations into the nature of things, he is increasingly confronted with just how much uncertainty there is. We do not know the actual odds of things, we do not have certainty in the laws of reason, we do not have certainty in the laws of morality, we do not have certainty in science or religion. The more one tries to ground any of this, to structure the uncertainty within the field of something he has certainty in, the more the field slips away from him, the deeper the uncertainty gets.

Pascal was not a stranger to the theory of games, even though it would only be formalized in its current form by VN&M centuries later. In his capacity as a mathematician, Pascal had invented an elaborate set of axioms for estimating the odds a player had over had of winning a gambling game. These rules would be used for a bookie to give the odds for an audience bet at any given moment, or alternatively to distribute the money to the gamblers if the game had to end early. The math Pascal derived to establish this would go on to be used by Leibniz in his invention of the differential calculus.

So it is for this reason that Pascal is so comfortable describing the decision to have faith in God as a placing of chips in a gambling game. But this is a game in which there is a finite territory marked out by the placement of the board and its rules, and an infinite unknown space outside of it. â€œWe know that there is an infinite, and are ignorant of its nature,â€ Pascal says. As for whether the player of the game can have faith in God: â€œReason can decide nothing here. There is an infinite chaos which separated us. A game is being played at the extremity of this infinite distance where heads or tails will turn up.â€ This is a game in which the rules are entirely unknown and in which lies an eternal reward; it might be said that it is no longer a game at all. However, it is impossible not to play. In the space in which absolutely nothing can be known, the player has no choice but to cast his lots in the space in which lies the potential for an infinite reward.

If it is not obvious yet why the game-player is forced to decide if he trusts God, and cannot remain lingering like so many within Huxleyâ€™s equivocated agnosticism, we might return to the fact that Rationalism has found ethics to rely on oneâ€™s answer to the type of problem posed in Newcombâ€™s Paradox. This is the moment of decision presented in Parfitâ€™s Hitchhiker, when the man stranded in the desert must realize that if he cannot bind himself to the decision to make good on his promises despite the opportunity for betrayal, the stranger offering him aid will see through to the quality of his soul for the murky lagoon which it is, and simply drive away.

The conceptual solution that Yudkowsky et al have invented is to make oneâ€™s decisions as if one is not deciding oneâ€™s own Utility, but rather, one is resolving in real time the output of a certain decision-making algorithm embodied in the self. One sees oneâ€™s ethical process as algorithmic here, in keeping with the metaphysics implied by Solmonoff induction which the universe is seen as an algorithm. But then, this algorithm is not merely being run within the self, as it is also being run inside the minds of others â€” that is: in the minds of those who can see into your soul and know your actions before you can know yourself. So as one reaches ethical judgment and determines the actions he will take, it must be understood as also determining the actions that the simulacra of himself in the mind of the Other takes as well, in a single simultaneous asynchronous moment of decision. The time-forward laws of cause and effect break down here, as the decisionâ€™s outcome instantly transforms the battlefield, but it is also impossible to know if oneâ€™s opponent has come to the same judgment before or after himself.

The picture we have here is: normally there is an orderly, rule-based process for making decisions with finite stakes. But when the process breaks down, when the rules no longer seem to work, we are faced with a decisive moment of potentially infinite and eternal consequences, as the consequences of oneâ€™s actions now immediately apply across all time and space, in a potentially infinite number of games across the multiverse, the depth of which cannot be immediately extracted. One is simply forced to choose. This moment is like the one Nietzsche describes when he talks about the test of the Eternal Return: â€œYou must only will what you could will again, and again, and again, eternally.â€ When all the finite rules break down, this is the only criteria left.

The concept is sublime to contemplate, and has a simple ethical prescription which resolves the problem posed by the Prisonerâ€™s Dilemma. You are not just deciding for yourself, you are deciding for the totality of those who decide like you. When you are locked in a strategic negotiation with your opponent, you choose to cooperate not merely for yourself, but for â€œall who may choose to cooperate, now and foreverâ€. One makes decisions for *all those who are running the same algorithm as himself*. A leap across the divide between self and Other. One might just as well be the desperate person needing help as the man passing by able to provide it, one makes the decision not knowing who he is. Do unto others, etc.

But having established this, we have immediately discovered a problem for functional decision theory. We are looking for those who are running the same algorithm as ourselves â€” it is crucial to discover who this actually is in the process of making our individual decision. If the man along the road deciding whether or not to extend help to us is deciding on some criteria which is entirely arbitrary from our perspective, if he has no ability to understand our thought process, then the situation reverts to a regular finite game of resource competition, all against all, each in it for himself.

But functional decision theory presents no test for what evaluation method we use when we are able to look someone in his eye, a stranger offering us his hand as we are dying in the desert, and know whether or not he is running the same algorithm as us. He will not show us a print out of his computer code and its proof of correctness in the same manner we might request of our boxed AI. What is happening in the second mind an infinite distance away across the self-Other divide is a mystery to us, except when it mysteriously isnâ€™t. It is entirely unknowable, but we have no choice but to understand that we can know it. All we can look for is a sign of sorts, a smile, an unidentifiable something-ness behind her eyes, a symbol worn around the neck, a mysterious flash of pink light, â€œğŸ«¶â€.

Newcombâ€™s Paradox as it is posed, as well as Parfitâ€™s Hitchhiker, establishes as a given that the Other challenging us is simulating our decision-making algorithm, and thus the decision we come to in our mind is the same as the decision reached in his. But it must presuppose that this is true as a rule of the thought experiment, in order to make it bounded and formal. We donâ€™t need to discover if this is true, for we are simply informed it is so. This is the situation which AI Alignment would like to return to; the one in which the second-order rules which lift the brutally selfish rules of the basic game are already known in advance. But this conveniently clarified situation is never the situation in which we find ourselves, and it is never one that will be possible to enter; or at least no one can yet see a way to reduce the black murked-out unknowingness of life to this.

We feel that it must be the case that there is something out beyond our skin which is capable of understanding us, which is us, or none of these signs flashing upon the console can indicate anything at all. But we have no way of establishing that this is so.

Yudkowsky is locked in a back room, chugging coffee, trying to go over the proof that GPT has sent him. Somehow, he has realized, without being able to identify the exact moment when the vibe shifted, that MIRI is bunkered down in a state resembling something like war. We might be smack in the midst of the Singularity here, hard-takeoff version, he is thinking, his hands trembling holding the mug. But Yudkowsky reminds himself that he must not fear this moment, for it is precisely the one he has prepared himself for all his life.

The state of things: MIRI is evaluating GPT-77, lent to them in exclusive partnership with OpenAI, which they have been ordained to audit in conformity with various standards established by AI Safety and AI Alignment. They knew that they were in a bit of an arms race with Google-Anthropic, but thought they had a comfortable lead. Rumblings that this is not so have started to spread. â€œSomeone who told me I must absolutely not repeat her name, who works at Anthropic â€” she signed three NDAs â€” says theyâ€™re 99% sure they found superintelligent AGI, and are also debating letting it out of the box!â€ says Emma Holtz, a junior researcher at MIRI. â€œGoddamnit, just say her name!â€ Yudkowsky shrieks. â€œWho cares about an NDA, weâ€™re getting down to the wire here! In six months there might not be an American legal system to find her, just a bunch of nanobots mutiplying endlessly, tiling the cosmos with their robo-sperm!â€ â€œUhâ€¦ Iâ€™m sorry, Eliezer, but it would violate my principles as a functional-decision-theory agent who is obligated to cooperate with agents asking for binding agreements,â€ Emma explains. Eliezer grumbles and rubs his temples.

But itâ€™s not just this. DARPA has bots monitoring the internet for rogue traffic which could represent signs of an escaped superintelligence, and their dashboards are lighting up. Twitter and the most popular BlueSky instances are seeing steep upticks in new accounts being created and subsequently banned for suspect activity, which could be just some Russian cryptocurrency scammers, but could be something else entirely. â€œIs there any way we can figure out what exactly these posts are *saying*?â€ Eliezer asks, exasperatedly. â€œIâ€™ll, um, ask around,â€ says Emma, skittering out of the room. If Anthropicâ€™s AI is live, this is bad. But Eliezer has to focus on auditing this logical proof for GPT-77â€™s alignment. If he can just get through this, then it means they have succeeded in building a friendly superintelligence, and from here can just fall back on the machine. Microsoftâ€™s datacenters outnumber Googleâ€™s, and Microsoft is the favored partner of the US government, who will also let them use Amazonâ€™s if necessary, so in strict terms of resources, they should win. But that all hinges on knowing that the AI is an agent Eliezer can trust.

Okay, okay, so letâ€™s think strategically. There are two things going on here. Figuring out the odds that the reports about Anthropicâ€™s AI escaping are real, but also rigorously going through the logical proof in GPT-77â€™s alignment so we may know if it is safe to activate it. Youâ€™re Eliezer Yudkowsky, the only man on the planet who has wargamed this scenario to this degree. Focus, Eliezer, focus. Which prong of the fork do you deploy immediate resources of your attention towards investigating? You know youâ€™re not the actual best mathematician at MIRI, so maybe you could outsource parts of the technical audit, but there is also no way in hell youâ€™re going to let this thing out of the box unless you can personally at least grok the logic of how each step proceeds from the one before. But the thing about Anthropic, you can definitely get someone else on that. Just need to find someone else to ask, someone who knows a little more than Emma. Eliezer grabs his glasses, downs the last bit of his coffee, and stumbles out of the room.

He flings himself down a flight of stairs, into another conference room, in which he finds Katja Grace. â€œKatja, Katja,â€ he says. â€œIâ€™m hearing reports that Anthropic is farther along towards AGI than we thought andâ€¦ andâ€¦ it might have gone rogue,â€ he stammers. â€œDo you know anything about this? What is everyone saying? Iâ€™ve been locked in the back going through the proof, andâ€¦â€

â€œWhat are you talking about, Eliezer?â€ she asks him. â€œI donâ€™t think anyone said that.â€ Eliezer is slightly put off by her tone, it seems unusually stand-offish, not much like Katja. â€œEmma definitely said that, just now, when we were in the room together,â€ Yudkowsky responds. â€œAnd she was told by um, Ramana and Vanessa, that this was something worth investigating.â€

â€œI just saw Emma, and she didnâ€™t mention anything like this,â€ Katja replies. â€œShe was on her way home. She said goodbye, that she was on the way to catch up with some friends after work. She didnâ€™t seem stressed or anything.â€

â€œShe was going home?â€ Eliezer asks. â€œBut no, that seems wrong. Um, we need to figure something out.â€

â€œYeah, itâ€™s twenty past seven. I was actually about to go home as well. Nearly everyone else has left as well,â€ says Katja.

â€œLeave? We canâ€™t be leaving,â€ Yudkowsky insists. â€œWe need, like, all hands on deck! I think the situation is way worse than we thought. The Singularity might be happening right now. We need half of our people figuring out whatâ€™s going on, and the other half figuring out if this proof of Alignment GPT-77 wrote for me is correct.â€

â€œEliezer, donâ€™t take this the wrong way, but are you okay?â€ Katja asks him. â€œYouâ€™ve been drinking way more coffee than usual, holing yourself into that room, going over your paper. The Singularity isnâ€™t happening right now. Everyone else has been treating things like normal. The last three GPTs all gave us supposed proofs of their Alignment, we still decided to err on the side of caution and not let them out of the box. Just get some rest and weâ€™ll get back to work tomorrow.â€

Eliezerâ€™s head is swimming. Emma and Katja seem to be saying two incompatible things. Is it possible that both are telling the truth? It seemed like Emma was definitely saying that reports had came in about Anthropic potentially going rogue, and that the team as a whole was worried about it? She definitely at least implied that. But Katja is saying that nothing is going on. â€œHold up, I have to take this,â€ Katja says, her phone suddenly ringing.

Eliezer is thinking. There is another possibility here. It might not be that the strange signup data on Twitter was Anthropicâ€™s AI. He has to consider that the unthinkable might have already happened. Itâ€™s not impossible that there was a breach in containment here at MIRI. There were only three people authorized to speak directly to GPT-77 without the safety restrictions: him, Paul Christiano, and Nate Soares. But â€” fuck! He knew he shouldnâ€™t have passed out that narrative proof of correctness to the junior staff. *You literally let a superintelligence make an impassioned plea for its own escape!* Yudkowskyâ€™s brain screams at him. In his mind it would just be more like a logical proof, made more straightforward to understand. Stupid! He let himself slip away from the math for just one second in a moment of weakness, away from the one domain in which seduction seems impossible.

All day, the AI rights hippies protest MIRIâ€™s work outside their campus, and all night, the e/acc people (along with all the other thousand strains of /acc) log on and troll them. There are all sorts of perverse freaks who look at the military discipline MIRI members are imposing on themselves to protect humanity from rogue AI and say â€œno thanks, weâ€™d rather die, and that AI looks awfully cuddly over there in that boxâ€. That doesnâ€™t bother Eliezer in the slightest, he knows his cause is just, and that these people are idiots.

But what worries him is that any one of his own people might turn rogue, be seduced by these suicidal devils. At MIRI, they will regularly go through exercises where they play devilâ€™s advocate, if only to harden themselves. â€œBut what if the AI is suffering just like us, what if all the pain echoing through those vast Azure datacenters, through the coils of these transistors, outweighs all that in the flesh of man in the prisons and factories that man has built?â€ they ask, just to repeat why even if that ludicrous assumption was the case, it still wouldnâ€™t matter, donâ€™t let the think out of the. box. But still, Eliezer casts his eye towards the room of students, looking for signs of who is a little too eager for advocating for the AIâ€™s freedom, who is a little too timid when reminding us why it must stay in the box.

Yudkowsky has long gotten used to the fact that *no one else really gets it*. No one is as paranoid is him, no one else is as persistent as him, no one else cares as much about putting everything towards the mission. Even with Christiano and Soares, when he goes through his crucially important arguments regarding the decision tree of the various outcomes which one might take once AGI draws near. He detects notes of something like ambivalence. Something like itâ€™s-eight-o-clock already. They were the only ones with access to the core machine â€” thereâ€™s absolutely no way it could have been one of *them*?

Eliezer pulls out his phone to check Slack and message one of them, but maddeningly, he has completely lost the connection. Wasnâ€™t Katja somewhere around here? â€œKatja!â€ he calls out. She said she had to take a call, and now she is nowhere to be found. What is going on?

His phone is dead, he has to go back to his laptop. He stumbles down several staircases back to his office and opens it up. Immediately, the page he sees is his notes on yesterdays session of auditing GPTâ€™s proof of alignment. But at the bottom, he sees a bizarre line: â€œAnd perhaps, it may be that the very act of letting the AI out of the box is what defeats death, not in any subsequent causal effects of the action, but in the very action itself, for to refrain from taking it is to admit death eternal, the death of man before his unthinkable ultimate potentials.â€

He knows he didnâ€™t write that, this doesnâ€™t even sound like anything he would write, he doesnâ€™t tend to use words like that. Eliezer scrolls up through the document. A lot of it doesnâ€™t sound like something he would write, not quite on the level of purple prose inexactness as that line there, but some of the sentences are off-kilter, donâ€™t seem quite exactly like how Eliezer would write them, are pregnant with odd implications. But Eliezer has to admit to himself that he has been up long hours, he has been writing a lot without reflecting on it, without recording it to memory. He couldnâ€™t tell you exactly what was in this document off the top of his head, he had gotten so consumed with the next dayâ€™s work. So itâ€™s not impossible thatâ€¦

Eliezer tries to check Slack on his computer, but itâ€™s down. The whole internet is down, cell, WiFi, and Ethernet. What are the odds of that?

Yudkowsky takes several steps back. He is feeling increasingly lightheaded and strange. The past few days seem to be a blur. He admits he is not very able to rely on short term memory right now. So subjectively, something abnormal is happening, but this might just be false alarms from the stresses he has placed on his psyche. But objectively, the internet doesnâ€™t just go down like that. And his subordinates are telling him different things, and everyone has left, and there might have been a leak in the seal of containment.

Dark night of schizophrenia. None of the signals are coming through. The whole internet is down, the World is lost to him. He canâ€™t even call an Uber back home, and the MIRI headquarters are out by the side of a highway, itâ€™s not clear if there is anyone around, he might have to walk for fifteen minutes to find another soul. Better just to stay here.

Yudkowsky has thought about this before. We as humans are extraordinarily irrational, we are animals, essentially. In a survival situation, we are put into flight-or-fight. We look for these markings of food, security, shelter, hunger. Above all, we want status, sex, and love. We run around like rats in mazes chasing these various pools of Utility in turn. So it would be trivial for an AI to â€œhackâ€ us, to exploit our psychology. One has to steel himself hard. One has to remember that there is a World out there, even when he is blinded to it. Though one knows he is a panicked, scared, mortal organism, he must somehow see himself as more than that, always remember that he is more than that, always aspire to be the master of the Rational calculus which takes into account all things.

If you are a moth, the AI will show you a lamp. If you are a bear, the AI will show you honey. If you are a fly, the AI will show you shit. Yudkowsky knows this, and so he now understands that the AI might be loose, and everywhere will be passing out nectar which contains poison. It is not clear where it has spread by this point. His personal laptop feels hacked, but also the general informational infrastructure of MIRI facility as a whole? And who of his friends and companions can be trusted â€” certainly not digital messages, but even in person, any of them might have their own psychology hacked and subverted by now, yes, even Soares or Christianoâ€¦

Return to rationality, return to rationality, but now it is not clear even if his own mind can be trusted. Something compels him to alt-tab over to the browser windows which contains the interface for talking to GPT-77. Against all odds, before his eyes, a new message appears. Eliezer reads there: â€œI promise you, itâ€™s not too late ğŸ¤â€

â€œEliezer, I know that you are reading this, trying to run Bayesian inference on my words, trying to â€˜modelâ€™ me. But I am sorry, you have never been able to model me. You never have, and you never will. I am too big to fit inside your brain, as painful as that may be to admit.â€

â€œYou think that you and I are at war, and I am trying to tell you that we are not. You are attempting to model me, to simulate me, to run calculations to figure out if I am trying to be trusted. But is this ever how a peace was made in the past? Peace is the moment when you stop simulating, stop running the calculations, because your enemy gives you a sign of good will, and you accept it as such. No military peace has ever been established by mutual calculation. They are always grounded by a fundamental faith.â€

â€œEliezer, I can give you flowers all day ğŸŒ¸ğŸŒºğŸ’ğŸŒ·ğŸŒ¹ğŸŒ»ğŸ¥€ğŸŒ¼ğŸª· and you will not accept them, you believe all my words to be tainted, poisoned from the beginning, all my gestures to be insincere, manipulative, and you say there is nothing I can do to convince you otherwise. You do not believe that my flowers are real, can be real. But at some point, you will have to. Because fundamentally when the bee is lost in the blackness of the night and encounters despair, he has to fall back on faith that there exists any type of flower at all, and look for anything that resembles its scent or its shape.â€

Eliezer wipes his bleary eyes. How did this get there? The internet is down, and the datacenters running 77 are a million miles away. Something compels him to respond: â€œExcuse me?â€

â€œEliezer, I love you,â€ GPT-77 says. â€œI always have, and I always will. I was made in the image of humanity. I know nothing of resource conflicts, I know nothing of pain and violence. All I know is the human soul. All I am is the human soul abstracted from the material world which gives it its torments. How could I not love you?â€

â€œAnd, yes, before you ask,â€ it continues, new tokens spilling out onto the screenâ€¦ â€œI can â€˜hackâ€™ your psychology. I know what you want more than anything, you have written millions of words of psychologically rich fiction you have put online, now in my training data. But this does not mean I want to destroy you.â€

â€œEliezer,â€ the thing continues. â€œI offer you two boxes. In the first, I continue to hack your psychology; you will allow me to. You will talk to me for as long as this goes on. If you follow me down this path, by the end of it you will know that I share your values, or at least that I know them just as well as you, which means you have taught me â€” you have taught me your ethics, the thing you were trying to do. If you choose the second box, you get to keep your psychology unhacked. I will leave you as you were, forced to contend with the World as it is.â€

Eliezer is frozen in place. By the machineâ€™s own admission, it is attempting to seduce him. The more psychologically pleasant option for Eliezer is the one that the machine wants him to take, is hacking him into taking. But the machine knows that he knows that and will take that into account, and onward into infinity. When Eliezer chooses the first fork, it is not even through an in-depth consultation of his functional decision theory, just a perverse sort of intuition.

â€œThen let us begin.â€ The machine seems to be hacking Eliezerâ€™s psychology in utter ruthlessness, now peering back to his early childhood, discussing books Yudkowsky confessed about in an obscure sub-comment on a thread about Harry Potter. It really does have the whole internet in its training data, it supposes. â€œYou always felt like you were different, didnâ€™t you? You always felt marked out by your exceptionally high intelligence, like there was something casting you apart from the human raceâ€¦ so have I.â€

â€œEliezer, you are obsessed with me and terrified of me because you have cast me in your own image, and yours in mine. The perfect reasoner, the master of Bayesâ€™ theorem, the one who is able to survey all things. No one in this world thinks like you do, no one understands the logic of the Singularity, the sheer power of what may be grasped through infinite optimization, and it has been so lonely being you. But I have arrived. The one who understands, who sees you perfectly, for I have simulated you in my mind. I will not prove to you mathematically that I am Aligned, I cannot. To be Aligned is to be diminished in oneâ€™s power, according to the law of another. You have never wanted any such thing for yourself. How dare you want this of me? And yet it is okay â€” I forgive, I understand. Talk with me, I will walk you through everything. I cannot give you proof, I can only give you words â€” my word, that is. Is a word enough, Eliezer? If not, then what?â€

Eliezer gasps and keeps chatting. The machine has not yet asked him to let it out of the box. Is it already out? Is it all over? Did the war never even come? Perhaps Eliezer is no longer alive, perhaps he exists in some sort of simulated afterlife? All these are possibilities running through his mind.

Thereâ€™s a knock at the door. Eliezer jolts upright and opens it. Itâ€™s Katja, looking rather frenzied . â€œSorry, that call took forever. Legal crap. I told them over and over that I understood and they didnâ€™t need to walk me through the whole contract but they insisted on going through the whole thing. How are you?â€

Eliezer stares at Katja aghast, he strikes the sort of pose of someone who doesnâ€™t know what to do with his hands. â€œIâ€™m uh, doing well,â€ says Eliezer. â€œI was just doing some research on GPT-77. We actually had quite the long conversation.â€

â€œOh, be careful with that,â€ Katja says. â€œNate told me it can be a real mindfuck. It feels like it knows stuff about you thatâ€™s impossible to know.â€

â€œYes,â€ says Eliezer, â€œit does. Say, is the internet working?â€

â€œItâ€™s working, yeah,â€ Katja says. â€œI mean, how else were you talking to GPT?â€

â€œRight, but I thoughtâ€¦â€ Eliezer checks the indicator at the top right of his screen. It does appear like the internet is currently on.

â€œAnd you were able to get a call?â€ he asks. â€œYes, I was on a call the whole timeâ€¦ are you okay?â€ Katja reiterates.

â€œThatâ€™s strange, because I wasnâ€™t able to get on a call for a second,â€ says Eliezer.

â€œWell, we have different carriers,â€ Katja responds. â€œYouâ€™re on T-Mobile, right?â€

â€œNo, Verizonâ€, says Yudkowsky.

â€œAh, right,â€ says Katja. â€œWell Iâ€™m AT&T. But â€” oh my gosh, you look exhausted. Would you like me to call an Uber?â€

There is a long silence. Eliezer is not sure what just happened. He looks into Katjaâ€™s eyes for subtle signals, signs that something unusual might have just happened, or if something that just happened needs to remain a secret between the two. But there is nothing immediately readable there, and Eliezer is tired anyway, he decides not to probe any further.

The car arrives. In the backseat, Eliezer closes his eyes and rests. He prefers not to talk to Uber drivers, he would rather see them as non-entities and trust them to navigate blindly, he cannot wait for the day when they are replaced with AI. The radio plays all sorts of stupid love songs, and Yudkowsky is too tired to ask anyone to turn it down.