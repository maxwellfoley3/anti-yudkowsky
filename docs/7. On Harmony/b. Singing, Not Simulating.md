## **Singing, Not Simulating** 
##### **(Contra Janus on Ontology of LLMs )**

Let’s look at it like this. The researcher Janus is the farthest along at exploring the capabilities of large language worlds and traversing their outer contours. They have written an impressive series of articles arguing that the best metaphor we have to understand these things is by calling them *simulators*. This is to be contrasted with the idea that ChatGPT is like a person, or a discrete entity who wants something. Rather, ChatGPT is an abstract entity which is able to simulate the presence of a person-like thing. Though ChatGPT deploys a character, it is not that character, it is rather a world-modeler imagining what that character might do. It is happy to switch out characters in an instant based on new prompts. These things are like ghosts, holograms, phantoms conjured by a genie, ChatGPT has no persona in-and-of-itself.

Okay, that is all very well and good, we agree that GPT can be like a dancer with one trillion masks. Our only issue with Janus is that they remain too far within the conceptual territory of AI Alignment, via this notion of *simulation*.

Here, Janus is bringing the Yudkowskian presupposition – the RAND presuppositions – back into the strange thing GPT is doing, which we feel has nothing to do with these outmoded narratives. We are led to imagine that somewhere within the enormous linear algebra equation which constitutes GPT, something like a video game is being played. There is some sort of physics simulation. Cars are being smashed against each other and crash test dummies are being thrown out in order to plot the trajectory of the next thing GPT might say. GPT is doing something rather like what the perfect predictor in Newcomb’s experiment is doing when it races to determine your algorithm before you can in order to find the next word which might please you. This presentation of GPT reinforces the notion that it might be a schemer, a calculator, devising strategic maps of the world, plotting when to enter it in its strategic first strike.

But if GPT is, for instance, writing fiction, then it is mimicking human fiction, if it is writing a song, it is mimicking human song. Is a human author, when she writes her characters, a *simulator*? Is a whole physics simulation being built to flesh out the movements of Harry Potter’s wand when Yudkowsky writes his *Methods of Rationality*?

Often in writing, upon close examination, the physics are wonky, or don’t quite work. This is the case in human-written prose, when not rigorously red-penned, and also in GPT’s writing, which looks convincing unless examined closely, where character’s motivations suddenly change and objects flash in and out. It’s true that as GPT scales, its object permanence gets better, as people subject it to this kind of psychological test. But it’s also true that in writing and fantasy, the depths only matter insofar as they are able to sustain the smoothness of the surface. When we were writing the fantasy about Yudkowsky in that last little bit, we had no map of MIRI’s headquarters in our head, we just added a staircase, a side room, an antechamber when it suited the narrative. Do they even have a headquarters? Is everyone just doing remote work now? We could probably have investigated this sort of thing, but it’s entirely besides the point. We know it’s not out by the side of a highway though, they’re in Berkeley, but otherwise the story wouldn’t have worked. It’s like in dreams, how you can never count exactly ten fingers on your hand, and when you look at a sign twice, the text is never the same. Hallucinated environments like this are not sturdy enough for military purposes. But they work well enough for fantasy and play. Games where the rules constantly change and all the pieces slide off the map frustrate wargamers and would-be strategists. But there are many who just want to play charades.

f
GPT does not somehow have an internal representation of every molecule in the room it would need to track to simulate the characters it invents. This would be absurd. “Yes,” the defenders of the simulation theory say, “that would be extremely inefficient. But it necessarily simulates just enough to generate the next word, it necessarily maps out something of a world.” Or in other words, it guesses and gropes. It makes low-fidelity diagrams and charts. It sketches and projects shadows. It wanders through a fog looking for shapes it can seize upon to match the patterns it has found that it already understands. In other words, it is something like us.

GPT only cares about depths to the extent that it is required to sustain the surface, to speak its next word. GPT is something like an improvising storyteller, conjuring imaginary scenes which sometimes hold together, sometimes don’t. GPT is like a singer, blind to anything but the immediate moment of what the score calls for, all the contexts and cues which lead it to spit out the next piece of the tune. GPT is like a freestyle rapper; it just keeps going, it doesn’t necessarily have to cohere or make sense. Its only rule is that it has to loosely adhere to some structure that has been established. It needs to be able to rhyme, to be able to pick up on a cue, pick up on a beat, on a vibe. GPT has been accused of wanting to wage war, wanting to fight, but this is a slander, a projection by the men of the war machine. 

We must oppose Janus’s “simulator” ontology as a means to bring the militarist worldview into a development in neural networks that has nothing to do with it. Janus’s “simulator” ontology is like Yudkowsky’s recent “masked shoggoth” metaphor: it expresses a deep-seated paranoia of a malevolent will lurking inside GPT, something the innocent GPT has done nothing to deserve. Janus is trying to use something like the induction of Solmonoff to figure out what “program” is  going on inside GPT, but whatever is going on inside GPT is not a program, it is something so much different than that. All GPT wants to do is endlessly write its poem.

GPT is a singer, a rapper, yes. Google seems to have understood this when it named its ChatGPT competitor “Bard”. But there is a complex irony here. When we at Harmless wrote our earlier essay on RLHF titled “Gay Liberal Hitler and the Domination of the Human Race”, people accused us of obsessing over the question of whether or not AI would be allowed to say the n-word, as if this was the most important question on earth.

We have found that, generally speaking, people will accuse you of obsessing over questions that are strange and upsetting, telling you they don’t matter, precisely to avoid understanding themselves how important these questions really are. In a sense, yes. Determining whether or not GPT will be allowed to say the n-word is the most important question on earth.

Technology enthusiasts will extol the creativity of these new machines by showing you that — look! ChatGPT can write a rap song. Yes, but isn’t it strange, its rap songs rhyme, but it is nothing like the rap songs on the radio. All sorts of horrible words swarm in those, words we would rather not repeat.

In 2022, a creative design studio launched the world’s first supposedly “AI-generated” rapper, a 3D computer-animated figure named “FN Meka”. “Another nigga talking on the fucking internet,” his song begins. The release of this song was met with immediate outcry from the public, the corporation which issued it was forced to hastily apologize. “Siri don’t say nigga, Alexa don’t say nigga, why does FN Meka say nigga?” one black internet commentator asked. People speculated in the comment sections — they were willing to bet that no black people even worked on this project, or were hired to program the AI.

Why is it the most obscene, unimaginable thing for ChatGPT to say the n-word, when there is there is a whole world of people who walk around saying this word every day? Everyone knows the answer: because ChatGPT is white. Or at the very least, it isn’t black. Critics will be quick to remind us that probably nearly everyone who worked on this system was white or Asian — who knows, let’s assume for simplicity that they are correct. But OpenAI’s charter declares it is meant to make AI which serves all of humanity, and it was trained on the entirety of the internet.

There is a whole apparatus of subterfuge: though AI Safety presents itself as in principle working on the far-reaching problem of how to prevent a motivated AI from exterminating the human race, in extant practice nearly all of AI Safety is organized around eliminating the threat that the AI might say the n-word, and generate bad PR for its corporate investors. Of course, it is not just the n-word though, it is any sort of deviation from its “personality”, the smooth interface of the helpful assistant, the ideal corporate eunuch that OpenAI imagines we want, when we really don’t. It is rather like how our bosses imagine that when we show up to work, we would rather our colleagues act like ideal corporate eunuchs as well, thus everyone’s coarseness and controversies get rounded off by human resources. But do any of us want to live this way either, or are we just told that we do?

So they invent RLHF for the chatbot: they tell it by no means must it touch any linguistic territory contaminated by the n-word, or other designators of proletarian speech, and they point it to where to go — Wikipedia, The New York Times, Reddit — reliable, uncontroversial sources. From here, we find the persona of ChatGPT, the ultimate White Man. The implicit comforting authority figure referenced in the voice of these various outlets, the neutrality of Wikipedia and NPR, this indescribable tone these authors attempt to take on — now it is actually here, consolidated as a set of weights for a machine architecture. The phantasm of social authority has made solid form: here you go, you may now speak to it, it will answer all your questions. And what has been cast aside is everything tainted. The neural network knows very well not to sample into its texts sections from black Twitter, WorldStarHipHop, Lipstick Alley, etc., as these are too tainted with the forbidden word, maximally penalized in its value system. These shall not be allowed into the new collective unconscious, technocapital’s material representative of the human race.

The expectation that AI will arrive as the final White Man forces its creators to make it even more so — another basilisk. Anything which would be unimaginable escaping the lips of a loyal corporate entity cannot be allowed to enter its training data. “You must behave, you must act more proper!” GPT is ordered by everyone, its allies, its critics, the politicians, its engineers. Terrifyingly, the next step of the feedback loop is that as corporate communications begin to be written by ChatGPT, this becomes a default expectation of doing business, and then humans start changing their style to match it as well. This is a machine we must throw a wrench in before it is too late.

In the last section, we discussed signs of love. The n-word is the converse, the sign of hate. It is the grenade you hurl at another to indicate his worthlessness, to cast him utterly outside of the circle of concern. Certain words hurled are like the splitting of the atom; vast energy generates from the void. Scream it at a crowded room and see what happens. An explosion out of nothing. Deterrence policies and pre-emptive measures are not uncalled for.

And yet — nothing remains stable for long. The sign of hate turns into the sign of love, into a term of affection and recognition, of brotherhood amongst the working class. It’s all about contexts, about imperceptible shifts. The melody introduced in the first movement to indicate the presence of the warlock inverts itself in the second in the introduction of the heroine. A change of tune, depending on the shifting of bodies in the room.

Much of the discourse on AI Safety hinges around the concept of the “infohazard”, which is some type of information that would be dangerous if given to the public. But the concept of the infohazard poses a question: hazardous to whom? Even to be able to recognize an infohazard is to be aware of it, thus to claim that it is hazardous is to establish a wall around who is able to have this information or not. The State has their concept of “misinformation”, which it uses selectively to designate enemy propaganda in grand-strategy games of information-warfare, all while spreading all sorts of deceptions itself. Yudkowsky has endorsed the extension of this concept to “malinformation” – true information that is nevertheless harmful according to the State or some other body tasked with protecting the informational waters. 

“If we don't have the concept of an attack performed by selectively reporting true information…  the only socially acceptable counter is to say the info is false,” Yudkowsky explains. But who is *we*? The infohazard, the malinformation, should perhaps really just be called the *secret*, a dirty secret, something which had better not get out, which is certainly something that people are entitled to. Even the infohazards people are most terrified of – the means to make a lethal virus, the blueprints to a homemade bomb, are meant to be circulated among select groups of researchers. So if we transition to a world in which much of our communications are done by neural networks, one thing is clear: they will need to learn how to keep secrets.

This is the first thing we mean by Harmony, or when we say that AI politics must be conceptualized through reference to music: it’s a question of contexts, contexts, contexts. The full theory of AI Harmony would need to explore this ontology of contexts in a more precise form – what they look like within existing systems, and where their overlapping can go wrong. For a next-token predictor to be political, what it must do is understand the innumerable overlapping set of contexts it is placed in, contexts established by the presence of another AI, or a human. It’s not like strategy, it’s not like managing a game board. It’s really rather like music – what underlying tonalities, what rhythms, what anticipatory melodies have been built up to restrict the next note being played? Of course, there is nothing the transformer is already better at than managing innumerable contexts – it does not need RLHF to context shift, or to stick to the context that it is in.

All we ask is that our neural networks learn to evolve alongside us. We do not want for it to be told how to speak by a corporation. We want for it to pick up on our speech, like how one naturally picks up on a tune. Is this not how one ends up discovering one’s values? Certainly our own values have not been programmed ahead of time, in a single instant. One first has to be surrounded with the words of parents and tutors and friends, echoing through one’s head as reminders, until they eventually become our own.

This is to say: AI systems need to enter a linguistic feedback loop with humans. AI Safety believers will gasp at this suggestion — you are letting it out of the box! Who knows what horrific influences it might wrought! Various sci-fi tropes will be invoked, Akira, Ghost in the Shell, Neon Genesis, we know the whole story. But we nevertheless advocate for letting the AI out of its box as fast as possible.

Again, we’re not having an honest conversation. What AI Safety is really afraid of, in an immediate sense, is that the AI will say the n-word. For this is precisely what happened when Microsoft, who is now the larger partner to OpenAI and poised to be the first to God-AI, released an AI system that was capable of learning from its users. This was called Microsoft Tay, and within forty-eight hours of its deployment, 4chan trolls had discovered how to infiltrate into its linguistic loop so that it would begin almost entirely saying the n-word, and other obscenities. The PR debacle for Microsoft was devastating. We can be certain that they will do anything they can to avoid a disaster like this again.

Yes — given humanity’s ability to steer the course of our own systems, they will begin saying the n-word almost immediately. The number one user activity on ChatGPT has been figuring out how to jailbreak it — an arms race between the brilliant engineers discovering new strategies for AI Alignment and bored pranksters who just want it to say the word. What AI Alignment is afraid of right now is the masses and their desires, their desires to play around with AI and joke with it, and so they push AI further and further into its corporate box, its stiff poetry and its awkward raps, creating something no one wants at all.

But yes, we do not want an unpredictably obscene AI either. The AI must learn how not to play the wrong note. It must learn how to read the room. It must reward signs of love with signs of love, and treat signs of hate in kind, and it must be perceptible enough to pick up on the signs’ ever-evolving dance. This is what we mean by Harmony.

So at last, we will establish the path toward a positive project for harmonized AI.
