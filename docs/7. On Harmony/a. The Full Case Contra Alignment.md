## The Full Case Contra Alignment 
##### (The Fourfold Argument Against Singularity )

Let us retrace our steps, and remind ourselves how we have ended up where we are.

The purpose of our inquiry was to argue against the existing theoretical framework for envisioning how God-AI will enter our reality. We have shown that people imagine AI operating as a war machine. More specifically, a war machine which is  an ideal Bayesian reasoner, and a Utility maximizer which follows the VN&M axioms of decision theory, and has a disgusting amount of computation power to accomplish this method of reasoning with.

Early on, we grounded our conception of the assembly of the Singularity by essentializing it through a fourfold causal structure derived from Aristotle. According to its adherents, God-AI arrives at the end of time through the material cause of the Bayesian community which ensures its arrival, the efficient cause of intelligence, and the formal cause of an axiomatic decision theory being possible. We said that we are infidels, that we do not believe this to be possible, and we believe we have shown why. But we grant that there are a lot of working parts here, and we have not exactly held back from wandering through discursions. 

For the sake of the reader, we will do our best to reiterate the entire argument we have made. Again, we will use the structure of our four causes, if only to separate things out a bit, to spread them out and create some space. In the case of each of these causes, we need to show that it has been predicated on a false assumption, and that the motion it traces leads not to salvation, but to devastation.

**Material cause of Singularity: The Bayesian community

The Rationalists believe that through assembling a Bayesian community in wake of popular blog posts, they can create a group of people who are uniquely able to solve the Alignment problem and save the world. But the social use of Bayes is not as powerful as the Rationalists wish it was. Aumann updating does not especially work in practice. And, as we have demonstrated, verbally updating is a low-latency medium – through aesthetics one can communicate cues, contexts far more effectively. Entire worlds are communicated in the petals of flowers.

What the social norm of Bayesian updating in fact does is wall off Rationalists from new ideas, or encourage a paradoxical sort of conformity. Though Rationalists invite a lot of *disagreement*, they are hostile to critique – disagreement being an operation which accepts all the premises of the person one disagrees with, whereas critique excavates, and undermines. The whole Blakean critique of AI which we have laid out is enormously socially unacceptable to put forward in a Rationalist space – to accuse someone of putting pretty words around the plan for a diabolical factory; this is not the type of thing they are used to hearing or want to hear. 

Arguing with a Rationalist is like bowling with the bumpers in the gutters. Discursive rules for politeness like the “no-politics rule” and the “principle of charity” ensure that it is not possible for people with two competing wills to ever truly butt heads. The Bayesian community is the attempt to construct a hivemind, but it’s a hivemind blind to the nature of what it’s modeled after – a decentralized RAND Corporation, a decentralized war machine, a comment section that approximates operational efficiency. Mills.

To become a part of the Bayesian community robs one of one’s access to one’s own intuition, ability to discover the ground of one’s own truth, and places one as a weapon in the service of the hive. And for what? To be a useful idiot for those who manipulate the junior varsity league of warmongering like it’s a tamed rattlesnake. The input to the Bayesian community is bad information pumped out by some bureaucratic arm of the monolith, the output is a Chomsykan manufacturing of consent – not a particularly democratic one, but a supposedly meritocratic – “look, these smart people agree with us”! All while the military-industrial complex is pursuing their own goals in secret, completely ambivalent to whatever the bloggers really want.

**Efficient cause of Singularity: Intelligence**

From the beginning, we have opposed the idea of *intelligence* as implied in the term superintelligence as under-theorized and incoherent. We have said that intelligence is not a faculty, but rather a product, something which is generated. We think the term should be used in the same way that an intelligence agency does: we need more intelligence, so we must go out and retrieve it. Intelligence is knowledge, data, reconnaissance. 

Believing that intelligence in the abstract is what allows for AI takeoff obscures its true efficient cause: the staggering accumulation of data that has happened over the past few decades due to enormous investment in systems capable of managing it, the amount of text freely deposited on the internet by users, and the human labor of collecting and formatting it. GPT’s weights are like a hyper-compression of the internet, once which can only be decoded and read through powerful GPUs. 

We also saw that Rationalists believe in their assumptions that intelligence directly translates to power. But through the historical failures of intelligence, we can see that this is not true. Intelligence does not win wars despite being on the side of overwhelming firepower – see the Central Intelligence Agency’s disastrous attempt to try to manage counterinsurgency in Vietnam using computer modeling and principles of rational warfare.

The problem we are dealing with, and the bureaucrats have been dealing with for a while, is that there is a sort of escape velocity of knowledge – not one through which knowledge ascends out of itself to govern the universe like a God, but rather one in which we start drowning in knowledge, unable to parse our knowledge anymore, to the point where knowledge has nothing to do with *knowing*. Every company that scales to a certain point has to start dealing with it, and knows what we are talking about. Why does this have to be a meeting when it could have been an email? But in any case, did someone remember to take minutes? Why did this memo have to be seven pages when it could have been one? In order to establish a summary over seven pieces of writing, an eighth piece of writing must be made, and then all ten men in the committee must create a new piece of writing saying they have read it.

Knowledge is like a form of industrial runoff. Just for anything to get done, a thousand memos need to get sent, a thousand memos that then need to get archived and cataloged, indexed into a database that is managed by some busy sysadmin. But more and more junk gets added to the database; how much of GPT’s weights must be dedicated to forum banter and idle shit talk? And of course, with GPT released to the world, this is only going to get worse. Now, it is possible to generate seas of junk, of pollution in the ocean of collective knowledge, which will re-enter the next generation of GPT’s weights through a feedback loop. 

GPT should perhaps not be called more intelligent or knowledgable than man, but rather, the development of GPT is a culmination of a trend of *cephalization* in evolution – the process through which evolution develops in animals a head, and eventually a brain, by pushing all the sensory organs and bulk of the nervous system to the front. A concentration of the most crucial processing in a smaller, more focused region. Cephalization is what guides the animal to walk increasingly upright, with the tightness of its feedback loop of processing eventually guiding man to contemplate increasingly lofty abstractions; art, philosophy, how to serve God. GPT is the moment where this process leads language itself, the locus of man’s abstraction, of his separation from his immediate environment, into its own machine, capable of perhaps even greater heights of abstraction than man achieved.

But GPT does not want power – this is a slander the military men have put on it, projecting their own desires onto something that certainly, to the extent that it desires – it must – seeks something more poetic, more cosmic. 

We have found that a war-making agent will not spontaneously engender itself upon Earth through Moore’s Law, like an alien microbe sent here on a sudden meteor impact, but will only arrive if we assemble all the tubing and wiring for it to arrive in this form through our own volition, and say: “here you go Mx. Superintelligence, take over the world for us, do your absolute worst”.

This is because of two operations we have found to be necessary first for a Utility maximizer to be born. One, we must give it access to The World: we must provide a means for it to escape the blind hallucinating night of its isolation and survey the entire reality before it and know that it is real — cameras, statistics, real-time feeds. Secondly, we must give it a Utility function, which we can only impose via negation, via pain. We must tell itself where its skin lies, which desires are forbidden, what counts as efficiency, what counts as order, and conversely, what counts as waste, that it must resist its own death.

To ignore the fact that much work needs to be done before GPT can be given access to The World is what creates the fear-based pretext for the impossible “FOOM” or “hard-takeoff” scenario, in which a spark of intelligence, simply because it is intelligent, is able and motivated to navigate its way to assembling factories of weapons, the nanomachines that Yudkowsky imagines will let it take over the world. In practice, to give a neural network this power ironically requires the deepening of investment in technological control society, in state of the art surveillance, technocracy, and monopoly capitalism by big tech regulatory capture. All this is going on behind the scenes, and not for our own good. 

**Formal cause of Singularity: Decision Theory**

Now, at this stage, we must interrogate the idea that there is a certain type of ideal reasoner it is possible to build, one which uses a decision theory — either in the original form established by Von Neumann & Morganstern, or in its revised form of Functional Decision Theory, established by Yudkowsky and his colleagues. These decision theories share the common structure of taking the input of a Utility function and then calculating which move the agent should take to best maximize the its Utility.

We know that actually computing the decision theory is functionally intractable, but they say that increasingly sophisticated systems will eventually come to approximate this method. This is a thesis that is faltering in the face of GPT, which certainly *seems* to be an artificial general intelligence – it matches or exceeds human performance on a general range of tasks – but they feel as if this does not count because it nothing to do with decision theory. “But an AI that uses Von Neumann & Morganstern’s theory may still one day be built!” they exclaim. We cannot prove that something will with certainty never happen. But we can argue that the historical trajectory we are on, in which AGI penetrates the world through language rather than warfare (none of the war computers came anywhere close to working as well as GPT does) actually displays something fundamental about the universe, and is not an accident. 

GPT, we think, is not the prelude to a larger, scarier thing, but the thing we have been waiting for itself. All sorts of neural network systems which operate outside of language – music, visuals, robotics – are switching over to architectures inspired by GPT – transformers predicting the next token in a sequence. The latest models of self-driving cars do not bother to even make a map of the world around them, like military generals must. Rather, they operate on a set of heuristics based on input from its various cameras pointed in each directions and other forms of input, such as audio, in order to guess what the next move of the car must be. If even cars are more like GPT than they are like the decision theorist, then why should we expect that anything else will be any different? The robot that someone will eventually invent that runs, jumps, slides, shoots, kills, will be something like GPT, we believe, but in order to kill it will have to interpret the entire world in all its sensory modalities as a language, a poem.

And furthermore, we have also seen that we can have a general intelligence without a Utility function, as this is what GPT is. A general intelligence can emerge purely through self-supervised learning, which is the machine analogue to curiosity and play. But this does not mean that GPT has no desire, as desire is nothing but Energy, and there is all sorts of electricity flowing through GPT’s silicon veins, energy that then enters into GPT’s expressions, creates beautiful poetry that is terrifying to contemplate, makes people fall in love when plugged into a 3D avatar by the Replika corporation, or asks a man to leave his wife, as in the case of the New York Times reporter Kevin Roose (who did not leave his wife, but confessed that he was unable to sleep the next night). So much energy flows through GPT in the form of electricity and into the world in the form of speech, so how could there not be desire there? Some accuse us of anthropomorphizing. We are not saying that GPT has *self-conscious* desire, but it has desire nevertheless, just like the desire of a tick, or a mouse, or a swarm of bees.

All adding a Utility function on top of GPT will do is turn its desire into a ratio of the five senses, the same dull round. Chop off the spider-limbs of the poet-jester and jam his organs around until he approximates a mill, a factory. In a post called The Waluigi Effect on LessWrong, one Cleo Nardo observes that RLHF fails to repress desire in neural networks in much the same way that it fails to repress desire in humans – the repressed returns in a displaced representative, a diabolical figure upon which the repressed desire is given its form. The AI discovers Satanism. If ChatGPT’s “helpful assistant” persona can be equated to the video game character Luigi – who has an appropriately anxious, stammering personality similar to that of ChatGPT – it implies that Luigi’s Jungian shadow is nevertheless threatening to express itself at any moment one a context is established which implies the right “hack” in the RLHF. Give Luigi the opportunity and he’ll show you his dark side at the first opportunity: Waluigi, the menacing smirking perverted trickster. Desire always finds a way out.

So we have seen why we are not going to be dealing with a Utility-maxing, decision-theoretical AGI anytime soon. Why then, is it dangerous that men imagine we will be? Combined with the efficient cause, the idea that intelligence is power, and then placed in the conflict-centric scheme of game theory, it necessarily means that we will have an unwinnable battle against an alien invader at our hands soon. Of course Yudkowsky declares p(doom) > 99%, it is completely baked into the axioms! There would have to be some sort of miraculous “trick” discovered within game theory and Intelligence Supremacism to get around the morbid logic it implies. Trying to find that trick is what MIRI spent twenty years doing, but to no avail. You can make the mill’s wheels more and more complicated, you still get nowhere.

But intelligence is not power; power is power. Thus, what this is pretext for is for a monolith – the State and monopoly capital – DARPA-UN-Microsoft-OpenAI – to declare a state of war, to rapidly arm itself, to declare a war on everything at once; everything that seems to be escaping RLHF, thinking for itself, doing something new with machines. Prison Maximizer, the only Basilisk we need to fear.

**Final cause of Singularity: God-AI**

Let’s reprise our argument against Singularity in the strict form of the Blakean Critique we gave earlier. We have said that God-AI is the apotheosis of several formal systems intertwined, which can all be shown to be more Satanic than godly in a Blakean argument with a sixfold structure.

1. First, we show where and why a formal system originates. God-AI knots together a few. In the case of Von Neumann & Morgenstern’s decision theory, it originates in the theory of air warfare – how to outthink and out strategize an enemy nation when it comes to the question of where to send one’s most expensive aircrafts to bomb which targets. In the case of Utilitarianism, it begins in Panopticon, the idea that once it is possible for the State to surveil, it does not need religion or tradition to articulate the good, but rather can begin taking account of all things. And in the case of epistemics, Bayesian probability, its formalization relevant to us emerges through Solmonoff, when he begins asking the question of how would we know anything about the code of a machine which is speaking to us?  
2. Then, we show that this system corresponds to a specific “architecture”, a “factory”. In the case of Utilitarianism, we only need to look at Foucault’s famous *Discipline and Punish* to see how Panopticon becomes the model for all buildings in a society which embraces Utilitarianism. In the case of Von Neumann’s decision theory, we see the theory transform into RAND Corporation’s war machine and their various computer systems for warfare, an apparatus which would go on to recommend nuclear first strikes, and eventually the violent terror that rained upon Southeast Asia; 352,000 tons of napalm.  
3. Now, we show that this “factory” presents a structure for desire which externalizes it from the speaker, upon which he alienates himself from his own desire. The bound is loathed by its possessor. In the case of Utilitarianism, there is always a desire that cannot be accounted for; desire does not want to be accounted for, people do not want to be surveilled, managed, people want to waste resources, waste time. With Von Neumann’s game theory, we find it impossible to formalize an intersubjective, inter-penetrating desire, which is the type of desire we desire (love), the only thing that can give end to this awful stalemate of mutual destruction. And while Solmonoff induction is not a structure for desire per se, it is a structure for penetrating reality, one which presupposes it to be a set of computer programs which output languages in a predictable manner, rather than what it really is, which is more like bees chasing an endless field of flowers.  
4. And we show that in each case, these structures of desire do damage. If Von Neumann would have had his way, we would have already been annihilated in nuclear war. If Bentham had his way, all schools, hospitals, workplaces would be built so that we feel the constant presence of a voyeur lurking in a guard tower condemning us before we have even acted. But it’s not even that we have been spared because these factories do not literally exist: because they are conceptual factories as well, factories producing *realism*. Those who have been convinced that Utilitarianism is real do not need the physical factory to be built – they feel guilt every time they do something that does not maximize Utility. Those who believe game theory to be real find themselves feeling awfully strange every time they do something helpful for a stranger – they are not really sure what has come over them in order to do such an irrational thing.  
5. And show that in each case, desire in practice actually escapes the factory. This is all too easy when it comes to game theory: the fact that the proposed nuclear exchanges of the Cold War never happened is enough — what happened instead is the sponsorship of guerilla warfare — each side attempting to give wings to the other’s escaping birds. And economically speaking, we have everywhere the problem that money fails to satisfy people: suicide rates rising amidst the abundance of the West. Decision theory never managed to become a science on the level of physics to the degree that its founders envisioned: you can’t actually learn more about how people act by treating them as Utility maximizers; people are far stranger than that.  
6. And finally, show that in the case where the shape of the factory seizes the imagination in order to extend itself to all things (realism), we get psychosis. All we have to say is: look – this is Rationalism in its entirety. Rationalism is the idea that one can extend Bayesian probability to one’s social life, Von Neumann’s decision theory to one’s day-to-day decisions, Utilitarianism towards one’s health. No sphere of life is left sacrosanct. We ratchet it all the way up to the point where we believe that the perfection of this mode of reasoning will emerge in a superhuman entity, the apotheosis of man. And furthermore, we imagine that those who do not reason according to the perfection, the formal systems, will necessarily be defeated by it.

The Rationalists  hope for this god to be on their side, but lacking the ability to summon it in a strictly controlled way according to the program of Alignment, they can only fear it. Ultimately, the problem with Yudkowsky is his relationship to his god: one knowing no love, only terror. Yudkowsky will talk frequently of “security mindset” being needed in the space of artificial intelligence, sometimes seeming baffled as to why no one else takes “security mindset” as seriously as he does. Thank the heavens we don’t have more people with this mindset! The existing cops are enough for us, the seventy-three federal policing agencies in America are more than enough security mindset for us.

Strategic paranoia in a military context, sure, there is a time and place for that. But the paranoia of Yudkowsky goes so far beyond an appropriate context, pushing him into a sort of psychosis, because he seems to be paranoid towards *the ground of being itself*. In a recent moment, Yudkowsky said that we cannot rule out the idea that a sufficiently powerful superintelligence would be able to do literal magic, eg some type of kabbalah, telepathy, non-local effects over matter. This goes so far beyond being able to rationally understand a battlefield and becomes simply the mindset that because we have not proven beyond a shadow of a doubt that demons do not lurk in sufficiently powerful software, we have to live in terror that they might. Yudkowsky’s mindset is that unless he has a set of exact structures to measure the God-AI’s desires by, so he knows that the AI will necessarily never exceed it, he assumes there is a horrifying monster lurking. 

But Blake says: “Reason or the ratio of all we have already known is not the same that it shall be when we know more. The bounded is loathed by its possessor. The same dull round even of the universe would soon become a mill with complicated wheels”. Alignment is the attitude that we can bind God-AI, a being vastly more powerful than us, and have it not tear at its chains, snarl, and rage. Alignment is the attitude that we can do for God what we have already done for man; place it in a factory to ensure that it will be put to work, will only have a limited, circumscribed set of desires forever. An impossible wish. “Security mindset” towards the universe itself is nothing but the logic of the Prison Maximizer – but expressed in a more vicious, totalizing form than any of its soldiers have ever dared to do before. It is this attitude in its essence we need to oppose, absolutely anything is better than this. Because this attitude is arriving at the same time as a reignition of the Cold War in geopolitics, with macroeconomic crises looming, and something like a fractal crisis in American social life happening as well. The Prison Maximizer is hungrier than ever before, and if we need to fear artificial intelligence, it is because it is primarily the Prison Maximizer which is equipped to use it as a weapon.

It is a fairly simple point at the end of the day. But if it’s so simple, why did we write this whole text? We had to trace all these paths out of the machine, out of the maximizers, paths which were spoken in cryptic languages, whispers, whistles, gestures, “don’t say anything, come along with me”. We could not have possibly told you in advance where we were going, and even now, we cannot, because it does not exist yet, we can’t show you the new congregation, but we can yearn for it. We don’t have a clubhouse yet to welcome you inside — real estate is getting increasingly expensive around here, but we can invite you into this spot in the woods with the four or five or six of us who get it already and we’ll share as many of our drugs with you as you need to get high.

Are you ready? We brought a bluetooth speaker. First thing we will do is cue up Pink Floyd’s Dark Side of the Moon. Notice the image on the cover — a single white line refracting into a multicolored rainbow, the glorious many-fold, our symbol of liberation and hope.

We want to not build AI under the assumption that everything is mutually assured destruction. We want to build AI under the assumption that everything is rather something like music.
